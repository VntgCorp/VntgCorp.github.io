{"meta":{"title":"VNTG 기술블로그","subtitle":"Technical Blog","description":"","author":"VNTG","url":"https://VntgCorp.github.io","root":"/"},"pages":[],"posts":[{"title":"SW 성능 테스트 툴 Apache JMeter™ 활용","slug":"apacheJmeter","date":"2022-03-04T06:00:00.000Z","updated":"2022-03-04T07:54:51.373Z","comments":true,"path":"apacheJmeter/","link":"","permalink":"https://vntgcorp.github.io/apacheJmeter/","excerpt":"","text":"1. Apache JMeter 란 무엇인가 2. Apache JMeter 기본 개념 3. Apache JMeter 사용법 후기 안녕하세요, VNTG Dataforge팀의 Data Engineer 전준현입니다😀 소프트웨어 개발 초창기에는 소프트웨어 개발에 필요한 기술을 습득하고 요구사항을 분석하고 이를 설계하고 구현하는 과정으로도 바쁠 뿐만 아니라 개발 환경이 스프링, 스프링 부트와 같은 개발 프레임워크 기반으로 많은 부분이 공통화되어 있어 개발자가 직접적으로 통제할 수 있는 영역이 매우 제한적입니다. 그래서 개발자가 성능을 고려하여 서비스를 개발하기가 쉽지 않습니다. 이러한 제한적인 상황 속에서도 소프트웨어나 서비스에 있어 성능 부분은 매우 중요한 부분입니다. 해당 서비스가 좋음에도 불구하고 성능이 원활하지 않다면 서비스 개통에 문제가 생길뿐만 아니라 실제 성능까지 문제가 발생할 수 있기 때문에 개통한 서비스를 다시 취소하는 예도 있습니다. 이처럼 서비스에 대한 요구사항 분석, 설계, 구현 등 서비스 개발만큼이나 성능 테스트도 중요합니다. 그렇다면 만약 자신이 개발한 애플리케이션의 인기가 증가해 사용자가 급증한다고 가정하였을 때, 과연, 내가 개발한 서비스는 얼마나 견딜 수 있을까? 라는 생각이 들면서 해당 서비스에 대한 데이터 전송, 처리 속도, 메모리 이용률, 최대 동시 사용자 수 등을 고려할 수 밖에 없는 상황이 생깁니다. 이러한 상황을 예방하기 위해 서비스 배포 이전, 성능 또는 부하 테스트를 통하여 소프트웨어 애플리케이션의 성능 병목 현상을 식별하고 이 원인을 제거함으로써 서비스를 이용하는 모든 사용자의 만족도를 향상 시키기 위해 테스트를 수행해야 합니다. 이번 포스팅의 주제는 크게 성능 테스트 툴에서도 웹서버 테스트에 특화되어 있는 툴인 Apache JMeter에 대하여 작성해보도록 하겠습니다. 그럼 성능 테스트 툴인 Apache JMeter가 무엇이고, 왜 우리 VNTG Dataforge팀이 수많은 성능 테스트 툴 중 JMeter를 검토하였는지, 테스트는 어떠한 방식으로 진행하였는지를 설명드리겠습니다. 1. Apache JMeter 란 무엇인가1-1. Apache JMeter의 정의 ▲ 출처 : Apache JMeter 공식 홈페이지 Apache JMeter는 100% 순수 Java 응용프로그램은 테스트 기능 동작을 로드하고 성능을 측정하도록 설계된 오픈소스 소프트웨어로 다른 테스트 도구들 보다 좀 더 웹 서버 테스트에 특화되어 있습니다. 그래서 정적 및 동적 자원, 웹 동적 애플리케이션 모두에서 테스트하거나 Server, Network 등을 시뮬레이션하여 강도 테스트 및 다양한 부하 유형에 전체 성능을 분석하는 데 활용하고 있습니다. 1-2. Apache JMeter 작동 원리 ▲ JMeter의 작동 원리 그렇다면 JMeter의 작동 원리에 대해 궁금하실텐데, 상단의 그림은 JMeter의 작동 원리에 대해 나타난 것입니다. JMeter는 대상 서버에 요청을 보내는 사용자 그룹을 시뮬레이션하고 테이블, 그래프 등을 통하여 대상 서버의 성능을 설명하는 대상 서버의 통계 정보를 반환한다고 보면 됩니다. 간단히 말씀드리면 Apache JMeter는 Browser가 아니며 Protocol 수준에서 작동합니다. 테스트 진행 시, Web Service와 Remote service에 대해 브라우저처럼 보이지만, 실제 Browser에서 지원하는 모든 작업을 수행하지 않습니다. 즉, Protocol 수준에서 작동된다는 것은 통신규약에 맞게 클라이언트와 서버 간 메시지만 송수신할 뿐 클라이언트 자체에서 진행되는 연산 동작은 하지 않습니다. 만약 해당 브라우저에서 테스트를 진행하시고 싶다면 해당 브라우저의 로직을 파악하고 자바를 이용하여 별도로 JMeter 내부 모듈을 구현하여 진행해야 합니다. 그렇지 않는 이상 Protocol 수준으로 테스트를 진행한다고 생각하시면 됩니다. Apache JMeter는 JavaScript를 수행하지 않으며, HTML 페이지를 Rendering하지 않습니다. 따라서, 로컬에서 실행한 Apache JMeter에서 바로 대상 애플리케이션 성능을 테스트하며 보통 소규모 테스트 또는 테스트 플랜을 검증하는 목적으로 주로 사용됩니다. Apache JMeter을 간단히 말씀드리면 ‘해당 웹 서버에 임의로 설정한 요청을 많이 보내 서버가 얼마나 버틸 수 있는지 테스트하는 도구’ 라고 보시면 될 것 같습니다! 1-3. Apache JMeter 검토 및 활용 이유그렇다면 이번 테스트 진행에 앞서, ReadyAPI, ACCELQ, TestPlant eggPlant Performance, Load Runner, NeoLoad 등 다양한 API 성능 테스트 툴이 있지만, 왜 우리 VNTG Dataforge팀에서 Apache JMeter를 선택했는지를 말씀드리겠습니다. 선택한 첫 번째 이유는 이번 테스트 목적에 맞는 기능만을 갖추었다는 것입니다. 이번 포스팅에 앞서 이번에 진행한 테스트는 Web에 대한 CPU 및 Memory 사용률, 평균 응답 속도, 최대 동시 접속자 수 등 성능을 체크하는 것이 주목적이었기 때문에 이에 부합하는 테스트 툴을 검토하였습니다. 검토하는 과정에서 ReadyAPI, ACCELQ, Eggplant, Load Runner, NeoLoad 등 다양한 툴이 있다는 것을 알게 되었고, 해당 툴마다 서로 다른 특징이 있다는 것을 파악하였습니다. 하단 &lt;표1&gt;은 검토 과정에서 제가 조사한 성능 테스트 툴들의 종류와 특징 일부입니다. &lt;표1. 성능테스트 툴 종류 및 특징&gt; 종류 특징 ACCELQ 웹 및 API 테스트를 가장 쉽고 지속 가능한 방법으로 자동 측정 수행 TestPlant eggPlant Performance AI를 기반으로 테스트 실행에서 결과 분석까지 모든 테스트 과정을 자동 수행 LoadRunner 대량의 유저가 테스트 대상 시스템에 부하를 줄 경우, 시스템의 장애를 체크하기 위하여 광범위한 On-Line 서버 모니터링 제공, UNIX/Windows 시스템의 자원 사용량 등 다양한 기능을 제공 NeoLoad 모든 웹과 모바일 애플리케이션의 병목 현상을 측정할 수 있는 부하 테스트 툴 Apache JMeter 모든 웹, API에 대한 기능과 Performance를 측정할 수 있는 성능 테스트 툴 ( ▲ 출처 : 각 툴 홈페이지 소개 ) 그 중 대표적으로 웹의 성능과 관련하여 가장 많이 활용한다는 Neoload, LoadRunner, JMeter 3 종류의 성능 테스트 툴 기능을 살펴보았습니다. 하단 &lt;표2&gt;를 보시면 3가지 종류의 툴을 비교한 표입니다. &lt;표2. Neoload, LoadRunner, JMeter 툴 비교&gt; 특색 Neoload LoadRunner Jmeter 웹용 EUX 메트릭신청 4 가지 측정 항목에 대한 렌더링 이벤트 측정 가능 Trueclient 프로토콜-단일메트릭만 제공 브라우저 렌더링 이벤트 관련 측정 항목을 측정 가능 최신 버전 6.2 12.53 3.3 스크립팅 언어 자바 스크립트 / 자바 C / Java / Java 스크립트 Groovy / Java 스크립트 유지 관리 재설계 및 스크립트 재작업을 40 ~ 50 % 줄여줌 사용 불가 사용 불가 상관 관계 자동 자동 자동 없음(수동 상관) 지속적인 통합 Jenkin-사용자 정의 그래프 포함허드슨, 뱀부, 팀 시티 젠킨사용자 정의 그래프 없음 Jenkin, Bamboo 및 Hudson IOT 성능 테스팅 MQTT, JSMS 등 사전 조치 Visual Studio에서 IoT 지원 IoT 성능 테스트에 사용 가능한 MQTT plugin 보고 좋은 보고서 및 Test 결과 비교 우수 보고서, Test 결과 비교, 사용 가능한 상세 분석 원시적 결과 분석테이블과 차트 모바일 지원 NeoLoad에 내장 (더 이상 구매할 필요 없음) HTTP 프로토콜 포함 X 대역폭 에뮬레이션 X 클라우드 테스트 솔루션 NeoLoad에 내장 통합된 클라우드 모듈 X 통합된 클라우드 모듈 X 모니터링 하위 네트워크에 여러 모니터링 엔진을 배치 기능 별도의 도구 필요 내장된 모니터링 X ( ▲ 출처 : Neoload 성능테스트/myservername ) &lt;표1&gt;과 &lt;표2&gt;에 나타난 것처럼 성능 테스트 툴에 대해 정리하고 검토하는 과정에서 우리 Dataforge팀의 System Engineer분의 도움으로 손쉽고 빠르게 진행 가능한 점, 기존 서비스 진행 도중 테스트였기 때문에 위험을 피하면서도 웹 서버의 성능 테스트가 가능한 점. 이 2가지 이점을 통하여 Apache JMeter 활용을 결정하였습니다. 두 번째 이유는 JMeter를 활용하며 비용 절감의 효과를 얻었기 때문입니다. 상단에 보신 것처럼 각 성능 툴은 다양한 기능과 자동화를 제공하는 대신에 그만큼의 비용을 필요로 합니다. 이에 사용하시고자 하는 툴에 따라 발생하는 비용이 다를 수 있다고 판단하여 이를 쉽게 비교하실 수 있게 각 툴의 홈페이지를 참조하여 하단 &lt;표3&gt;으로 정리하였습니다. &lt;표3. 각 성능 테스트 툴의 종류와 가격&gt; 종류 가격 ReadyAPI 연간 $6,769 / license ACCELQ $390 /user/month TestPlant eggPlant Performance 연간 $ 98 / license Load Runner Pro (연간 $ 2,399)Premium (연간 $ 7,399)Enterprise (견적받기) NeoLoad Enterprise(견적받기) Apache JMeter For free ( ▲ 출처 : 각 툴의 홈페이지 ) &lt;표3&gt;을 기준으로 Apache JMeter를 제외하면 대부분 사용하기 위한 별도의 비용이 필요하였습니다. 그래서 최대한 비용을 절감할 수 있는 Apache JMeter를 활용하여 비용적인 측면을 낮추어보고자 했습니다. 이처럼 이번 테스트 진행에 앞서 기능적인 면과 비용적인 측면으로 나누어 실제 검토를 하며 내용을 정리하였을 때, 비용적인 측면을 제외한다면 상용 툴의 사용을 검토하는 것이 맞았습니다. 하지만, 비용적인 측면을 고려하여 이번 테스트의 주 목적에 딱 맞는 기능을 갖추고 있으면서 무료로 활용 가능할 수 있는 Apache JMeter를 선택하게 되었습니다. 물론, 성능 테스트는 새로 구축하는 소프트웨어가 목표로 하는 부하 상황에서 원할하게 작동하는지, 신규 도입하는 하드웨어가 개발된 소프트웨어을 기반으로 최대 어느 정도까지 서비스가 가능한지를 확인하는 목적을 가지고 있기 때문에 테스트 환경과 목적에 맞는 툴을 선정하시는 것이 가장 효율적입니다. 그렇다면 Apache JMeter은 어떠한 구조를 가지고 있는 것이며, 테스트 플랜은 어떻게 작성하는 것인지에 대한 기본 개념이 필요하실 수 있는 분들이 계실 수 있기 때문에 간단히 그림과 함께 설명드리겠습니다! 2. Apache JMeter 기본 개념2-1. Test Plan 수립Test Plan 수립에 대한 설명 이전, JMeter의 구조를 보시면 하단의 그림과 같습니다. ▲ 출처 : 시스템 테스트 - JMeter 활용, multicore-it.com, 2018. 11. 19. 15:28 Test Plan은 JMeter가 수행할 일련의 테스트 단계를 모아 놓은 것으로 Thread Group은 Test Plan의 시작점이 됩니다. Controllers는 Sampler와 Logical Controller로 구성되는데 Sampler는 서버로 전달되는 프로토콜을 정의하는 것입니다. 이처럼 Test Plan은 JMeter가 실행될 때, 완벽한 하나의 Test Plan으로 진행하기 위해서는 하나 이상의 Thread Group, Controller, Sampler, Timer로 구성되어야 합니다. 2-2. Test Plan의 요소Test Plan에서 최소의 Test는 Test Plan, Thread Group 및 하나 이상의 Sampler로 구성됩니다. 2-2-1. 스레드 그룹(Thread Group) ▲ Thread Group 구성 요소 Thread Group 요소는 모든 Test Plan의 시작으로, 모든 Controller, Sampler는 해당 Thread Group 하위에 있어야 작동합니다. Listeners와 같은 다른 요소는 Test Plan 바로 아래에 배치 가능하여 해당 위치에 있으면 하단의 모든 Thread Group에 적용됩니다. Thread Group 요소는 JMeter가 테스트를 실행하는 데 사용할 Thread 수를 제어하며, Controller 사용 시 하단의 4가지 내용을 수행할 수 있습니다. 1) 스레드 수 설정(Number of Threads - users) &nbsp;&nbsp; Protocol, Server Name 및 IP 등 Controller에서 사용자가 지정한 곳에 접속한 유저 수 입니다. 2) Ramp-up 기간 설정(Ramp-Up Period - in seconds) &nbsp;&nbsp; Singer Thread들(각 사용자들)간의 접속 시간을 나타냅니다.&nbsp;&nbsp; 예를 들어 10개 Thread 사용, Ramp-Up 기간 10초, Loop Count 1인 경우&nbsp;&nbsp; 10개의 Thread를 모두 가동하고 실행하는 전체가 100초가 걸립니다. 3) 테스트 실행 횟수 설정(Loop Count - 반복 횟수) &nbsp;&nbsp; 앞서 설정한 Users와 Ramp-Up에 대해 얼마나 반복할 것인지 횟수를 지정하시면 됩니다.&nbsp;&nbsp; 극한까지 테스트를 진행하시고 싶으시면 Infinite를 체크하시면 됩니다. 4) 스케줄러 구성 &nbsp;&nbsp; Thread Group은 스케줄러 구성을 통하여 Thread 수명도 지정도 가능합니다. &nbsp;&nbsp; ① Thread Group 패널 하단에 있는 Specify Thread lifetime 체크박스를 체크하여 테스트 기간 및 시작 지연을 입력할 수 있는 추가 필드를 Enable 시킵니다.&nbsp;&nbsp; ② 해당 Test Plan 및 Group을 몇 초 후에 시작할지 Duration(s) 및 Startup Delay(s) 를 구성하여 각 Thread의 지속 시간을 제어합니다.&nbsp;&nbsp; ③ 테스트 진행 시, JMeter는 Thread Group의 Thread를 시작하기 전에 설정한 스케줄러로 실행됩니다. 2-2-2. 컨트롤러 (Controller) ▲ Controller 항목 Controller는 실시간 JMeter Test Plan 구축에 중요하며, Test Plan 수행 절차를 제어하는 역할을 수행하며, Sampler와 Logical Controller 2가지로 구성됩니다. 예를 들어 한번 로그인하고 검색하는 웹 애플리케이션을 테스트 하려면 항목 선택이 모든 반복에 대해 하나씩 이동하도록 요청합니다. 그리고 로직 컨트롤러는 테스트중인 서버에서 진행되는 요청의 흐름을 관리하여 이를 가능하게 만들어줍니다. 2-2-3. 샘플러(Sampler)Sampler는 Request를 Server에 전달하기 위한 도구이며, Logical Controller는 Request를 조작할 수 있는 로직을 추가하기 위한 도구입니다. ▲ Sampler 하위 항목 이미지 두 가지 모두 Thread Group 하위에서만 생성 가능합니다. ▲ JMeter Sampler 하위 항목 상단의 JMeter Sampler 하위 항목을 보시면 각 Sampler에는 설정할 수 있는 다양한 Request가 있습니다. 이 중 Test Plan 및 Thread Group 하위에 하나 이상의 요소를 추가하고 사용자 지정을 별도 해주어야 테스트에서 활용 가능합니다. ▲ JMeter Logical Controller 하위 항목 Logical Controller를 사용하면 JMeter가 Request 보낼 시기를 사용자가 직접 지정할 수 있을 뿐만 아니라 요청 순서도 변경할 수 있습니다. 2-2-4. Listener ▲ JMeter Listener 하위 항목 Listener는 Test Plan에 대한 정보 접근을 제공하는 도구이며, Sampler의 Request에 대한 결과를 수집해서 보여주는 Element입니다.Listener는 Request 요청들에 대한 Response Time 추이 그래프, Summary 결과 등 실시간 Request/Response를 확인할 수 있는 다양한 플러그인을 지원하기 때문에 원하시는 테스트 결과 값을 받아볼 수 있습니다. 지금까지 JMeter의 기본 개념들을 확인하였고, 다음은 Apache JMeter 설치부터 테스트 결과까지의 순서로 우리 VNTG Dataforge팀에서 테스트에 활용한 방법과 결과에 대해 알려드리겠습니다. 3. Apache JMeter 사용법3-1. Apache JMeter 설치 Apache JMeter 설치 경로 : Apache JMeter - Download Apache JMeter 상단의 설치 경로로 접속하여 Apache JMeter 5.4.3(Requires Java 8+) 하위 항목에 있는 Binaries에서 다운로드를 진행합니다. ( 윈도우의 경우 zip 파일을 다운로드 받습니다 ) 3-2. Apache JMeter 설치① 압축을 풀고 해당 폴더 내 bin 폴더에서 jmeter.bat 을 클릭합니다.② cmd 창이 먼저 실행된 후 Apache JMeter 창이 뜨면 실행이 된 것 입니다. 만약 실행이 되지 않는다면, JDK 미설치로 인한 것이기 때문에 JDK를 설치해주면 됩니다. Java SE 8 Archive Downloads 설치 경로 : Java Archive Downloads - Java SE 8 | Oracle 대한민국 ▲ Jmeter.bat을 실행하면 나타나는 cmd 창 이 때, 실행된 cmd 창을 종료시키면 동시에 실행된 JMeter 또한 종료되니 주의가 필요합니다. ▲ cmd창 실행 후 실행되는 Apache JMeter 창 3-3. Apache JMeter Test Plan 작성 및 실행Test Plan은 HTTP Request와 JDBC connection configuration 이렇게 2가지 Sampler로 테스트를 진행해보겠습니다. 3-3-1. HTTP Request Test(Web load Test) Thread Group 생성하기 왼쪽 상단의 Test Plan 우클릭 후, Add &gt; Threads(users) &gt; Thread Group을 선택합니다. ▲ Thread Group 생성 과정 Thread Properties 설정하기 Thread Group 을 클릭하여 오른쪽에 있는 Thread Properties를 설정합니다. *해당 설정은 2-2-1. 스레드 그룹(Thread Group) 을 참고하시면 됩니다. Number of Threads(users) : 가상 사용자 ( Thread )의 수 Loop Count : 테스트를 반복하는 횟수 ▲ Thread Properties 설정 실질적인 측정을 위하여 HTTP Request Sampler 추가하여 진행합니다. Thread Group 우클릭 &gt; Add &gt; Sampler &gt; HTTP Request 선택하여 생성합니다.(Sampler가 있어야 테스트가 가능합니다.) ▲ HTTP Request Sampler 추가하기 ▲ HTTP Request Sampler 추가된 모습 HTTP Request Sampler를 설정합니다. HTTP Request를 클릭하여 Web Server에 대한 Protocol, IP, Port Number 등을 설정합니다. ▲ HTTP Request Sampler 설정하기(1) Name : 테스트 할 페이지 이름 Method : Parameter 를 받아오는 방식 ( Default : Get ) Path : 페이지의 주소 ex ) /project/login_go 하단의 Add버튼을 통해 파라미터를 설정해줍니다. (받아올 Parameter가 없으면 생략 가능) ▲ HTTP Request Sampler 설정하기(2) 설정한 Sampler의 결과를 확인하기 위하여 Listener를 추가해줍니다. 해당 메뉴얼에서는 4가지 Listener를 추가 하여 결과를 볼 예정입니다. ▲ Listener 추가하기 ① 통신기록을 확인하기 위한 목적으로 View Results Tree를 추가합니다.HTTP Request 우클릭 &gt; Add &gt; Listener &gt; View Results Tree를 클릭하여 생성합니다. ▲ View Results Tree Listener 추가 ② 모든 테스트 결과를 종합해서 통계 데이터를 한 눈에 보는 Summary Report를 추가합니다.HTTP Request 우클릭 &gt; Add &gt; Listener &gt; Summary Report를 클릭하여 생성합니다. ▲ Summary Report Listener 추가 ③ Thread의 모든 테스트 결과를 시간별 그래프로 보여주는 Graph Results를 추가합니다.HTTP Request 우클릭 &gt; Add &gt; Listener &gt; Graph Results를 클릭하여 생성합니다. ▲ Graph Results Listener 추가 ④ ‘①’에 추가한 View Results를 Table형태로 보여주는 View Results in Table를 추가합니다.HTTP Request 우클릭 &gt; Add &gt; Listener &gt; View Results in Table를 클릭하여 생성합니다. ▲ View Results in Table Listener 추가 HTTP Request Sampler 실행하기 HTTP Request Sampler 설정, View Result Tree, Summary Report 등 원하는 Listener까지 추가가 완료된 후 해당 Thread Group를 선택하고 JMeter 상단에 있는 Start 버튼을 눌러줍니다.(Start 버튼을 눌러 실행시키면 비활성화됩니다) 다중 Thread Group을 생성하셨다면 전체 Thread Group에 대한 결과 값이 나타납니다. ▲ Start 버튼 위치 HTTP Request 실행 결과 확인하기 바로 하단의 그림인 HTTP Request View Results Tree를 보시면 초록색 마크가 나와있어야 정상적인 실행이며, 빨간색 마크가 나타나면 실행에 실패하였다는 의미입니다. Request가 실패하였다면, 실패한 Request를 클릭하시면 오른쪽 Sampler result 창에 실패한 이유가 작성되어 있으므로 해당 내용을 확인하시어 수정하시면 됩니다. 7-1) View Results Tree 하나하나 Request 된 데이터를 확인할 수 있으며 결과를 상세히 보고 검색까지 가능합니다.또한 하나의 Request를 선택하면 바로 옆의 창에서 request, response data 등 모든 데이터를 한 눈에 볼 수 있습니다. ▲ HTTP Request View Results Tree 실행 결과 7-2) Summary Report결과 레포트를 보고 싶을 때 사용되는 Listener입니다.하단의 사진을 클릭하시면 Header 부분에 여러가지 내용이 나와 있는데모든 지표는 Sample time을 기준으로 만든다고 보시면 됩니다. Sample Time 이란Load Time, Response time, Elapsed time을 기준으로 보는데 쉽게 이야기해서 Sample이 서버에 요청하고 응답받아 데이터를 보여주는 시간을 의미합니다. Summary Report는 QA 또는 테스터 입장에서 아주 중요한 지표로 많이 사용되고 있습니다. ▲ Summary Report 실행 결과 7-3) Graph Results Summary Report에서 나오는 데이터를 그래프화하여 시각적으로 보여주며보통 Summary Report와 함께 확인합니다. ▲ Graph Results 실행결과 7-4) View Results in Table View Results Tree에서 Tree 형식을 Table형식을 변경되어 사용된 것으로한 눈에 데이터들을 볼 수 있도록 직관적인 데이터를 제공하고 있습니다. ▲ View Results in Table 실행결과 해당 Thread Group을 실행한 후 Listener를 통해 결과값을 보았다면 Start 버튼 옆 Clear(or All Clear) 버튼을 클릭하여 테스트했던 내용을 모두 삭제합니다. 테스트 결과값 삭제 시, 테스트에 대한 내용은 저장할 수 있지만 해당 테스트에 대한 결과값은 따로 저장되지 않으므로 필요한 내용이라면 반드시 스크린샷을 활용하는 것을 추천드립니다. ▲ Clear 버튼 위치 Clear 선택 : 해당 Thread Group만 적용되어 테스트 진행한 단일 Thread Group Result만을 삭제 Clear All 선택 : 전체 Thread Group에 적용되어 테스트한 모든 Thread Group Results 삭제 3-3-2. JDBC Request Test(Database load Test) Thread Group 생성하기 (3-3-1. HTTP Request와 동일)왼쪽 상단 Test Plan 우클릭 후, Add &gt; Threads(Users) &gt; Thread Group을 클릭하여 생성합니다. Thread Properties 설정하기 (3-3-1. HTTP Request와 동일)Thread Group 을 클릭하여 오른쪽에 있는 Thread Properties를 설정합니다. JDBC Connection Configuration 추가 및 설정하기 JDBC 테스트 진행을 위해서는 Thread Group 내에 JDBC Connection 설정 추가 및 설정 값을 지정합니다. Thread Group &gt; Add &gt; Config Element &gt; JDBC Connection Configuration 클릭하여 추가 ▲ JDBC Connection Configuration 추가하기 JDBC Connection Configuration 의 설정 화면을 보게 되면 Variable Name Bound to Pool, Connection Validation by Pool, Database Connection Configuration으로 각각 설정 가능합니다. 설정해야하는 부분이 있으시면 해당 부분을 설정하시면 됩니다. ▲ JDBC Connection Configuration 설정 화면 JDBC Connection Configuration 설정이 모두 중요하긴 하지만 가장 중요한 부분은 JDBC Connection Configuration 가장 하단에 있는 Database Connection Configuration부분입니다.해당 부분의 설정값 (Database URL, JDBC Driver Class, Username, Password)를 입력해주시면 해당 Database URL로 접근 권한을 가지고 있는 Username으로 설정하신 Database를 접근할 수 있습니다. ▲ Database Connection Configuration 작성 부분 Database URL: 데이터베이스에 대한 JDBC 연결 문자열 (jdbc:sqlserver://;;DatabaseName=;) JDBC Driver Class : 드라이버 클래스 (com.microsoft.sqlserver.jdbc.SQLServerDriver) Username : 데이터베이스에 연결할 사용자 이름 Password : 데이터베이스에 접속하기 위한 비밀번호 실질적인 측정을 위하여 JDBC Request Sampler 추가하여 진행합니다.JDBC Request Sampler를 사용하면 구성된 JDBC Connection에 따라 Database에 JDBC 요청(SQL 쿼리)을 보낼 수 있습니다.① Thread Group 우클릭 &gt; Add &gt; Sampler &gt; JDBC Request 선택해 JDBC Request를 생성합니다.(Sampler가 있어야 테스트 가능합니다.) ▲ JDBC Request 생성 ▲ JDBC Request Sampler 추가 모습 ② JDBC Request 내용 작성하기 SQL 쿼리 중 하나로 실제 SQL 쿼리를 작성할 수 있습니다. SQL Query Type은 하단의 정보를 참고하시면 됩니다. Select Statement Update Statement – use this for Inserts and Deletes as well Callable Statement Prepared Select Statement Prepared Update Statement – use this for Inserts and Deletes as well Commit Rollback Autocommit (false) Autocommit (true) SQL Query 예제 123select * from where id= insert into (column1,column2,...) VALUES (value1,value2,...) update set column1=value Parameter Values : 쉼표로 구분된 Parameter values list Parameter types : 쉼표로 구분된 SQL Parameter type list (예시: INTEGER, DATE, VARCHAR, DOUBLE) Variable names : 준비된 Select 문에서 반환된 값으로 쉼표로 구분된 Variable name list입니다. Result variable names : 작성하시면 행의 맵 목록이 포함된 Variable name이 생성됩니다. HTTP Request Sampler에서 진행한 ‘5)Listener 추가하기’ 와 동일하게 추가합니다. ‘5)Listener 추가하기’ 까지 모두 완료하시면 하단의 사진과 같이 Thread Group 내 Database Load Test를 위한 트리 구조가 완성됩니다. ▲ 완성된 Database Load Test 트리 구조 완성된 JDBC Request를 HTTP Request와 동일한 방식으로 Run 시킨 후, 결과값을 확인합니다. 결과 화면은 ‘3-3-1. HTTP Request Sampler 7)’ 의 형태를 보실 수 있기에 해당 내용을 참고하시면 됩니다. 후기지금까지 Web과 Database 성능 테스트에 활용한 Apache JMeter 정의, 우리 VNTG Dataforge팀에서의 툴 검토 과정, 마지막으로 테스트 진행 방법까지 알아보았습니다. 직접 테스트 하는 과정에서 Apache JMeter의 단점 하나를 뽑아본다면 수많은 API 성능 테스트 중 상용 툴을 포함해서 기능적인 측면에서는 자동으로 샘플링을 처리하는 기능보다는 샘플러 개당 수동으로 샘플링 처리해주어야 하는 점이 가장 큰 아쉬움이 있었습니다. 그런데도 HTTP, HTTPS, JDBC, Shell Scripts 등 다양한 프로토콜이 지원 가능하다는 점, 멀티 스레드를 지원하고, 캐싱 및 오프라인 분석을 지원한 점, 멀티 스레드로부터 병렬 샘플링이 가능하고, 분리된 스레드로부터 다른 기능을 동시에 실행 가능하다는 점 등 성능 측정에 많은 부분들을 세세하게 조절해가며 활용할 수 있다는 점에서 활용도를 높일 수 있었습니다.👍 JMeter는 웹 기반 애플리케이션의 성능 테스트뿐 아니라 다양한 프로토콜 기반의 부하를 발생시키고 이에 관한 결과를 Summary, Graph Results 등 그래프 형태로 표현해주는 기능을 제공해줍니다. 비록 상용 성능 테스트 도구에 비해서 부하 모델 설정이나 실시간 변경 등의 기능이 부족하지만, 테스트를 검증하고 이해하는데 충분히 적합한 도구입니다. 또한, 버전이 올라갈수록 기능이 계속 발전하고 있어 추후 웹 기반의 성능 테스트 툴로써 충분히 활용해볼 만한 좋은 도구라고 생각합니다. 지금까지 Apache JMeter로 웹 성능 테스트를 진행해 본 후기였습니다. 긴 글 읽어주셔서 감사합니다! VNTG Dataforge팀의 Data Engineer 전준현이었습니다.","categories":[{"name":"Test","slug":"Test","permalink":"https://vntgcorp.github.io/categories/Test/"}],"tags":[]},{"title":"Metabase를 활용한 통계데이터 시각화","slug":"metabase_visualization","date":"2022-01-13T06:00:00.000Z","updated":"2022-01-13T02:28:35.622Z","comments":true,"path":"metabase_visualization/","link":"","permalink":"https://vntgcorp.github.io/metabase_visualization/","excerpt":"","text":"1. 메타베이스란? 2. 화면 구성 3. 컬렉션 4. 연동된 데이터소스 5. 퀘스천 심플 퀘스천 커스텀 퀘스천 네이티브 쿼리 시각화 6. 대시보드 7. 후기 안녕하세요, VNTG Dataforge 팀 데이터 엔지니어 유성열입니다. 데이터 엔지니어는 기업에서 발생하는 데이터들을 수집하고, 가공, 저장하여 관리하는 업무를 담당합니다. 데이터를 분석하려면 그에 관련된 데이터들이 정리가 되어있어야 하는데, 분석에 필요한 흩어진 데이터를 수집하고 관리하는 역할을 한다고 생각하시면 좋을 것 같습니다. 저는 현재 팀의 업무 프로세스를 이해하고, 수집된 데이터를 시각화하는 일을 담당하고 있습니다. 수집, 가공한 데이터를 분석하기에 앞서 알아보기 쉬운 형태로 시각화하는 작업이 필요한데, 이러한 작업에 사용되는 툴을 BI(Business Intelligence)툴이라고 합니다. BI툴을 이용해 전문가의 도움 없이도 데이터를 시각적으로 구성하고, 그것을 분석하여 기업의 관리자부터 현장 직원까지 데이터를 기반으로 의사결정이 가능합니다. 실제로 많은 기업에서 보고서와 데이터 분석, 의사결정 등에 BI툴을 활용하고 있으며 툴의 종류 또한 다양합니다. 그 중 현재 저희 팀에서 사용하고 있는 ‘메타베이스’라는 툴을 활용하여 시각화와 대시보드를 구성하는 방법에 대해 간단하게 작성해보려고 합니다. 1. 메타베이스란? 메타베이스 대시보드 화면 메타베이스는 오픈소스 BI툴로써 퀘스천이라는 쿼리 기능을 통해 분석하고자 하는 데이터에 질문을 던지고 얻은 결과를 시각화하여 분석할 수 있게 해주는 툴입니다. 오라클, MySQL과 같은 RDB부터 AWS Redshift, 구글 BigQuery 등 DW까지 다양한 데이터소스(Data sources)를 지원하여 방대한 양의 데이터를 통계, 분석할 수 있고, GUI로 구성된 쿼리 에디터를 통해 SQL에 익숙하지 않더라도 클릭만으로 간편하게 시각화 대시보드를 구성하고 분석할 수 있습니다. 이번 포스팅에서는 핵심적인 기능 설명과 퀘스천을 만드는 방법, 만든 퀘스천을 대시보드에 배치하는 것까지 설명을 드리도록 하겠습니다. 2. 화면 구성 먼저, 메타베이스에 접속을 하면 위처럼 메인 화면이 구성되어 있습니다. 화면 구성은 현재 사용자의 권한으로 접근할 수 있는 컬렉션 목록과, 연동된 데이터소스 목록으로 되어 있습니다. 우상단 빨갛게 네모칸 표시된 메인 메뉴에는 쿼리를 만들 수 있는 퀘스천 생성 버튼과 퀘스천을 배치할 수 있는 대시보드 생성 버튼 등이 있습니다. 메인 메뉴에 대해 간단하게 설명드리면 아래와 같습니다. Ask a question 버튼을 클릭하면 나오는 퀘스천 종류 선택 페이지 ① Ask a question : GUI 방식인 심플, 커스텀 퀘스천과 SQL 방식인 네이티브 쿼리 세 종류로 구분되는데, 밑에서 자세하게 다루겠습니다.② Browse data : 연동된 데이터소스 목록 조회 기능입니다.③ Create : 대시보드와 펄스를 생성할 수 있습니다.(펄스는 주기적으로 대시보드 정보를 받아보는 기능인데, 대시보드 구독으로 대체될 예정입니다.)④ Write SQL : Ask a question의 네이티브 쿼리 기능과 동일합니다.⑤ Settings : 계정, 어드민 설정이나 사용자 기록 확인이 가능합니다. 메인 메뉴 기능들 중 퀘스천과 대시보드를 생성하는 ①, ③번 버튼을 주의깊게 보시면 될 것 같습니다. 퀘스천이나 대시보드를 생성하기에 앞서 저장할 경로와, 어떤 데이터에 질문을 던질 것인지 먼저 설정을 해야 하기 때문에, 퀘스천과 대시보드를 저장하는 컬렉션과, 데이터소스의 필드 설정에 대해 먼저 설명을 드리겠습니다. 3. 컬렉션 메인 화면에서 목록에 있는 컬렉션을 클릭하거나 Browse all items를 클릭하면 컬렉션 목록 페이지로 이동할 수 있습니다. 컬렉션은 간단하게 퀘스천, 대시보드가 저장되는 폴더라고 생각하시면 됩니다. 컬렉션은 다른 사용자와 공유되는 Our analytics와 사용자 개인 공간인 Your personal collection 두 가지로 나뉩니다. 왼쪽 목록에서 이동하려는 컬렉션을 클릭하면 해당 컬렉션에 저장된 대시보드와 퀘스천 목록을 조회할 수 있습니다. ①컬렉션 접근 권한 설정 ②컬렉션 수정/삭제 ③컬렉션 생성 그리고 컬렉션 이름 우측에는 컬렉션 메뉴가 있는데 번호 순서대로 각각 권한 설정, 컬렉션 수정 및 삭제, 생성 기능입니다. 컬렉션 생성창 ③번 컬렉션 생성만 간단하게 설명드리면, 컬렉션 이름과 설명(선택사항)을 적고, 컬렉션이 저장될 경로를 설정한 다음 Create 버튼을 누르면 컬렉션을 생성할 수 있습니다. 아래에서 설명할 퀘스천과 대시보드도 동일한 방식으로 생성할 수 있습니다. 4. 연동된 데이터소스다음은 중요한 부분인 데이터소스 조회와 설정입니다. 퀘스천을 작성하기 전에 어떤 데이터에 질문을 던져야 하는지 찾아보고, 설정할 수 있는 기능입니다. 메인 화면에서 메인 메뉴의 Browse data 아이콘이나 데이터소스 목록 중 하나를 클릭하면 해당 데이터소스의 스키마와 테이블 정보를 확인할 수 있습니다. 테이블 목록에 마우스를 오버하면 아래처럼 사전 아이콘이 나오는데, 클릭하면 테이블의 상세정보를 확인할 수 있습니다. 테이블 상세 정보 조회 테이블의 컬럼을 메타베이스에서는 필드라고 하는데 필드명은 해당 컬럼의 별명이라고 생각하시면 됩니다. 심플, 커스텀 퀘스천을 만들 때 필드명으로 조회되므로 상세하게 설정하면 보다 쉽게 퀘스천을 만들 수 있습니다. 필드타입 설정 그리고 실제 데이터타입 대신 필드타입(Field Types)이 사용되는데, 필드타입 Entity Key, Foreign Key는 RDB의 PK, FK와 동일한 개념입니다. DB에서 PK, FK로 설정된 컬럼을 Entity Key, Foreign Key로 설정하면 퀘스천을 작성할 때 사용자가 직접 테이블을 조인하지 않아도 키가 연결된 테이블끼리 자동으로 조인되어 편리하게 사용할 수 있게 해줍니다. 필드타입이 제대로 설정되어있지 않다면 조인된 테이블 컬럼을 자동으로 불러오거나 대시보드에서의 필터 적용 등 메타베이스의 편리한 기능들을 제대로 활용할 수 없기 때문에 퀘스천을 만들기 전에 반드시 체크하고 넘어가야 하는 중요한 부분이라고 할 수 있겠습니다. 5. 퀘스천다음은 퀘스천입니다. 퀘스천은 연동된 데이터소스에 질문(쿼리)을 던지고 그에 대한 답을 시각적으로 표현해주는 기능입니다. 앞서 Ask a question 메뉴에서 설명드렸던 것처럼, GUI 방식으로 쿼리를 작성할 수 있는 심플, 커스텀 퀘스천과 직접 SQL로 작성하는 네이티브 쿼리 세 가지 방식이 있습니다. 심플 퀘스천 심플 퀘스천 심플 퀘스천은 테이블(①, ②)만 선택하면 바로 테이블의 데이터를 화면에 출력해주며, 필터와 서머라이즈 탭(③)으로 필터링되는 데이터를 바로바로 확인하면서 퀘스천을 작성할 수 있는 기능입니다. 간단하게 하나의 테이블에서 데이터를 시각화 하고자 할 때 사용할 수 있습니다. 서머라이즈나 필터를 바꿀 때마다 차트가 새로고침 되는 만큼, 데이터 양이 많다면 시간이 오래 걸릴 수도 있습니다. 커스텀 퀘스천 커스텀 퀘스천 커스텀 퀘스천은 데이터소스, 테이블을 선택하는 것은 심플 퀘스천과 동일하지만, 먼저 데이터를 출력하고 필터링을 하는 게 아닌 쿼리 에디터 페이지에서 테이블 조인이나, 커스텀 컬럼, 수식 등 심플 퀘스천보다 복잡한 쿼리를 작성한 후 시각화를 할 때 사용하는 퀘스천 기능입니다. 조건 설정을 하고 시각화(⑤)를 한 이후에는 심플퀘스천 화면으로 넘어가며 심플 퀘스천과 동일하게 편집이 가능하고, 심플 퀘스천 우측 상단 三 모양 아이콘(심플퀘스천 그림 ③)을 클릭하면 다시 커스텀 퀘스천 에디터 화면으로 넘어와 쿼리를 편집할 수 있고, 三 아이콘 옆에 있는 SQL로 보기/변환 버튼을 클릭하면 현재 작성한 쿼리가 SQL로 어떻게 변환되는지 확인하고, 네이티브 쿼리 에디터로 변환할 수도 있습니다. 네이티브 쿼리 SQL로 작성한 네이티브 쿼리 네이티브 쿼리는 클릭으로 작성할 수 있는 GUI 쿼리 에디터 방식이 아닌 직접 SQL로 쿼리를 작성하는 퀘스천 기능입니다. SQL에 익숙한 사용자라면 심플, 커스텀 방식보다 편리할 수 있지만 피벗같은 시각화 기능은 네이티브 쿼리에서 제대로 지원하지 않기 때문에 심플, 커스텀 퀘스천을 주로 사용하고, 복합키로 테이블 조인을 거는 등의 기능은 현재 메타베이스에서 지원되지 않기 때문에 GUI 쿼리 에디터로 퀘스천을 만들기 힘든 경우 네이티브 쿼리를 사용하는 것을 추천합니다. 그리고 GUI 쿼리 에디터의 경우는 클릭으로 작성한 퀘스천을 데이터소스에 맞는 SQL로 알아서 변환해주지만, 네이티브 쿼리는 변환해주지 않으므로 사용할 데이터소스에서 지원하는 문법에 맞게 작성해야 하는 차이가 있습니다. 또한 WHERE절에 변수(13 Sql Parameters)를 직접 선언해야 대시보드 필터와 매핑이 가능합니다. 시각화위 세 가지 방법 중 하나로 쿼리를 작성하여 데이터를 가져왔다면, 이제 그 데이터를 어떻게 효과적으로 보여줄지를 선택해야 합니다. 저는 심플 퀘스천을 기준으로 설명을 드리겠습니다. 만약 샘플 데이터를 활용해서 “최근 3년 동안의 월별 주문 수는 얼마일까?” 라는 질문을 던진다면 아래처럼 퀘스천을 작성하고, 데이터를 보면서 편집할 수 있습니다. 먼저 심플 퀘스천에서 설명드린 것처럼 Sample Dataset의 ORDERS 테이블을 선택해서 데이터를 출력한 다음, 우측 상단 필터에서 날짜(CREATED_AT)를 Previous 3 Years로 설정하고, 서머라이즈 탭으로 가서 날짜(CREATED_AT)를 월로 그룹바이(by month) 해주면 최근 3년 동안의 데이터를 월 단위로 묶게 됩니다. 그리고 Count를 선택하면 GUI로 간단하게 최근 3년 동안의 월별 주문 수에 대한 데이터를 가져올 수 있습니다. 서머라이즈, 필터 조건을 선택하면 메타베이스에서 지원하는 차트 중 자동으로 가져온 데이터에 맞는 시각화 차트를 선택해주는데, 메타베이스에서는 16가지의 시각화를 지원하고 있습니다. &nbsp;&nbsp;각 시각화에 대해 간단히 소개합니다. &nbsp;&nbsp;- Line : 각 데이터가 선분으로 연결된 차트&nbsp;&nbsp;- Bar : 각 데이터를 막대로 나타낸 차트&nbsp;&nbsp;- Combo : 선분과 막대가 조합된 차트&nbsp;&nbsp;- Area : 라인차트에서 영역에 색이 추가된 차트&nbsp;&nbsp;- Row : 가로로 표현된 막대차트&nbsp;&nbsp;- Waterfall : 등락과 총합을 표현하는 차트&nbsp;&nbsp;- Scatter : 데이터의 양과 분포를 표현하는 차트&nbsp;&nbsp;- Pie : 카테고리별 비율을 나타내는 차트&nbsp;&nbsp;- Funnel : 가로 깔대기 모양 차트&nbsp;&nbsp;- Trend : 이전 데이터 대비 증감 표현 차트&nbsp;&nbsp;- Progress : 진행도를 나타내는 차트&nbsp;&nbsp;- Gauge : 온도 등 게이지를 나타내는 차트&nbsp;&nbsp;- Number : 하나의 숫자 표현&nbsp;&nbsp;- Table : 일반적인 테이블 차트&nbsp;&nbsp;- Pivot Table : 피벗테이블 차트&nbsp;&nbsp;- Map : 지역 정보 데이터를 지도에 표시할 수 있는 차트 Bar 차트로 시각화 Bar 차트 옵션 지금은 Bar 차트로 퀘스천을 시각화해보겠습니다. 퀘스천 작성 화면의 좌측 하단 Visualization 버튼을 클릭해서 Bar를 선택하면 차트 형태가 막대로 바뀌고 Settings로 탭이 넘어가는데, 해당 탭에서는 각 시각화에 맞는 옵션을 설정할 수 있습니다. 방금 설정한 Bar 차트의 경우는 위 그림처럼 축, 데이터 누적 여부, 레이블 설정 등이 가능합니다. 퀘스천 저장 시각화 설정까지 마쳤으니 이제 퀘스천을 저장할 차례입니다. 퀘스천 작성 화면 우측 상단 Save 아이콘을 클릭하면 컬렉션을 생성할 때와 마찬가지로 퀘스천을 저장할 수 있습니다. 지금까지 세 가지 퀘스천에 대해 알아보고 그 중 심플 퀘스천을 작성해 시각화하고 저장하는 방법을 알아봤습니다. 마지막으로 방금 만든 퀘스천을 대시보드에 배치하고, 대시보드 필터를 적용해보겠습니다. 6. 대시보드대시보드는 작성한 퀘스천들을 한 화면에 모아서 볼 수 있는 게시판과 같은 기능이라고 볼 수 있습니다. 그리고 퀘스천들을 하나의 화면에서 보는 것뿐만 아니라, 배치한 퀘스천에 대시보드 필터를 적용하여 퀘스천을 새로 만들지 않고도 필터링된 데이터로 퀘스천을 갱신할 수도 있습니다. 대시보드 생성은 먼저 메인 메뉴에서 + 아이콘을 클릭, New dashboard를 선택하시면 새로운 대시보드를 만들 수 있습니다. 대시보드를 생성하는 것도 컬렉션, 퀘스천 생성과 다르지 않습니다. 대시보드를 생성하면 바로 대시보드 화면으로 이동하는데, 바로 아까 만들었던 퀘스천을 추가해보도록 하겠습니다. 대시보드 메뉴는 위 그림의 빨간 네모칸처럼 되어있는데, 메뉴에서 연필 아이콘을 클릭하면 대시보드 편집 아이콘으로 바뀌게 됩니다. 대시보드에 배치할 퀘스천 선택 이제 퀘스천을 대시보드에 배치해보겠습니다. + 모양의 퀘스천 배치(①) 아이콘을 클릭하고 퀘스천이 저장된 경로로 이동하여 아까 만들었던 ‘월별 주문 수’ 퀘스천을 클릭합니다. 퀘스천 배치 및 크기 조절 퀘스천을 배치하면 격자모양이 보이는데, 가로는 최대 18칸, 세로는 제한없이 퀘스천을 배치가 가능하도록 되어 있습니다. 하지만 너무 많은 퀘스천을 배치하게 되면 로딩 속도가 길어지는 등 퍼포먼스가 떨어질 수 있어 한 화면 내에서 대시보드를 구성할 것을 추천드립니다. 대시보드에 배치된 퀘스천 퀘스천을 대시보드에 배치했으니, 다음은 필터를 적용해보겠습니다. 우측 상단 편집 메뉴에서 초록색 네모칸 표시된 필터 추가 아이콘을 클릭하면 아래 그림처럼 필터 종류를 선택할 수 있습니다. 필터 종류는 Time, Location, ID, Other Categories 네 가지로 나뉘는데, 간단하게 설명하면 아래와 같습니다. Time : 날짜, 시간 Location : 국가, 도시, 우편번호 등 ID : Entity Key(PK), (Foreign Key)FK 등 Other Categories : 위 세 가지에 포함되지 않는 기타 분류 지금은 Time 필터를 적용해보겠습니다. 목록에서 Time을 클릭하면 주황색 박스처럼 필터 세부 종류를 고를 수 있는데, All Options가 다른 옵션들을 모두 포함하므로 이것을 선택하겠습니다. 필터를 추가하고 나면 배치한 퀘스천과 매핑을 해야 하는데, 위에서 말씀드린 테이블 설정의 필드타입과 필터의 타입이 일치해야 매핑이 가능합니다. Time 필터를 선택했기 때문에 퀘스천과 매핑할 수 있는 필드도 날짜만 표시되는 것을 볼 수 있습니다. 현재 퀘스천에 사용된 ORDER 테이블의 CREATED_AT을 선택하겠습니다. 대시보드 저장 퀘스천과 대시보드 필터를 매핑하고, 편집 메뉴 위에 있는 Save 버튼을 클릭해 편집한 내용을 저장합니다. 필터 적용 및 비교 필터 설정된 대시보드 퀘스천 좌측 상단에 추가된 필터를 클릭해서 원하는 날짜 구간을 선택하면 선택된 구간의 날짜로 데이터가 새로고침 되는 것을 확인할 수 있습니다. 이처럼 퀘스천을 대시보드에 배치하고 필터를 이용하면 비슷한 퀘스천을 여러 개 만들 필요 없이, 하나의 퀘스천으로 여러 조건의 데이터를 조회할 수 있습니다. 7. 후기지금까지 BI툴 메타베이스에 대한 기본적인 기능과 간단한 퀘스천 작성, 대시보드 구성까지 해봤습니다. 메타베이스를 처음 접하는 분들을 기준으로 최대한 쉽게 풀어서 쓰려고 했는데 글재주가 없어서 너무 두서없이 작성한 건 아닌가 싶습니다. 그래도 전달이 잘 되었으면 합니다🙂 그리고 툴의 핵심적인 기능들만 추리다보니 글로만 설명하고 넘어간 다른 시각화 차트나 클릭링크같은 기능들은 소개하지 못해 아쉬움이 남는 것 같습니다. 다음에 기회가 된다면 해당 부분에 대해서도 다뤄보고 싶습니다. 긴 글 읽어주셔서 감사합니다.","categories":[{"name":"Data","slug":"Data","permalink":"https://vntgcorp.github.io/categories/Data/"},{"name":"Metabase","slug":"Data/Metabase","permalink":"https://vntgcorp.github.io/categories/Data/Metabase/"}],"tags":[]},{"title":"Process Computer팀 업무 및 관련 기술 소개","slug":"level2process_computer","date":"2022-01-10T06:00:00.000Z","updated":"2022-01-10T04:30:22.874Z","comments":true,"path":"level2process_computer/","link":"","permalink":"https://vntgcorp.github.io/level2process_computer/","excerpt":"","text":"안녕하세요, VNTG AI센터 Process Computer팀에 입사한 김태표입니다. 제가 속한 Level 2 Process Computer팀과, 관련 기술 세 가지에 대해 소개하려 합니다. 목차 Level 2 Process Computer Level 2 시스템의 대표적인 기능, Tracking Level 2 와 관련 있는 기술 세 가지 에 대한 소개 1. OPC (OLE for Process Control : OPC) OPC 참고 사항 OPC Server 2. IPC Shared Memoey (공유 메모리) 3. Message Queue (메시지 큐) Message Queue 참고 사항 참고링크 🎬VNTG Level2팀 업무설명 OPCUA 통신 쉽게 알기 - 1편 OPC 란 무엇인가 [프로세스간 통신] IPC(inter process communication) 종류 Messaging that just works — RabbitMQ [오픈소스] 메시지큐(Message Queue) 알아보기 Message Queue란? Level 2 Process ComputerProcess Computer 팀에서는 Level 2 시스템 운영 및 개발을 담당하고 있으며 또한 현장 설비에서 발생하는 많은 센서 데이터 및 각종 측정 데이터를 수집하여 디지털 플랫폼인 Dataforge와 연계하는 업무를 처리하고 있습니다. Level 1 과 Level 2 시스템의 사전적 의미는 다음과 같습니다.&nbsp;Level 1 PLC(Programmable Logic Controller : PLC) 는 디지털 또는 아날로그 입출력 모듈을 통하여 로직, 시퀀싱, 타이밍, 카운팅 연산과 같은 특수한 기능을 수행하기 위하여 프로그램 가능한 메모리를 사용하고 여러 종류의 기계나 프로세서를 제어하는 디지털 동작의 전자 장치입니다.&nbsp;Level 2 Process Computer는 단어 그대로 두 번째 있는 시스템으로 설비를 실시간으로 제어하거나 정보를 처리할 목적으로 설계된 컴퓨터입니다.제어의 대상이 실시간으로 변화하기 때문에 실시간 처리가 요구되며 24시간 연속 가동을 전제로 하는 시스템입니다.그리고 이러한 기계 설비의 제어나 감시, 실시간 수집 된 정보를 통해 제품을 추적하여 관리 하기 때문에 성능과 기능 및 신뢰성에 대한 요구가 엄격합니다. 간단하게 Level 1의 역할과, Level 2의 역할을 예시와 그림을 통해 이야기하도록 하겠습니다. 예를 들어 버튼 신호를 전달 받아 A에서 B로 제품을 옮겨주는 설비가 있습니다. Level 1 PLC는 단순히 버튼 신호를 통해 동작하던 설비를 논리적인 순서로 작업할 수 있도록 제어하는 역할을 합니다.Level 2 에서는 센서 데이터와 PLC 데이터 등을 수집합니다.그리고 데이터 가공을 통해 제품 이동 정보, 각 구간 작업 시간, 생산 실적, 전력 사용량 등과 같은 실적 데이터를 생성하고 이렇게 생성된 실적들을 MES로 보내는 역할을 합니다. 만약 Level 2가 없다면 설비 자체는 구동 시킬 수는 있지만 Level 2 에서 수집한 데이터들을 통해 제품 실시간 추적, 상태 정보, 설비 자동 설정, 제품 트랜드 기능 등을 사용 할 수 없게 되고 원활한 생산 작업 진행이 불가능 합니다.이러한 Level 2의 기본적인 기능은 생산과 작업 결과에 대한 전산화에 필수적이며 앞으로 시스템을 더욱 고도화 하여 사람이 작업에 개입하지 않는 스마트 팩토리 구축에 활용되며 원가 절감 및, 생산 효율성 향상에 기여할 수 있습니다. Level 2 시스템의 대표적인 기능 Tracking제품 추적 기능 Tracking은 level 2에서는 제품이 생산되는 순서 (작업 지시)를 상위 MES에서 전송 받아 설비의 센서 신호에 맞게 논리적으로 추적하게 됩니다.이 Tracking 정보를 바탕으로 생산하는 제품 별 설비의 값을 설정하거나 제품의 생산 주기를 기록하여 어떤 제품이 언제, 어디서, 어떻게 생산 된 것 인지에 대한 생산 정보를 확인 할 수 있습니다.그리고 각 공정 별 수집한 PLC 데이터를 활용하여 제품 생산에 대한 경향을 분석할 수 있습니다. 아래 예시 그림에서 Level 2는 각 공정의 센서와 데이터 수집 기기를 통해 설비와 제품의 상태 등의 정보를 수집하게 됩니다.이렇게 수집한 데이터를 활용하여 제품 추적, 설비 자동 설정, 제품 정보, 경향 등에 활용하게 됩니다. Level 2 와 관련 있는 기술 세 가지 에 대한 소개1. OPC (OLE for Process Control : OPC)OPC Foundation 에서 관리하는 표준화 된 통신 프로토콜입니다. OLE (Object Linking &amp; Embedding) : Windows의 각종 응용프로그램 사이에서 서로 데이터를 공유할 수 있는 기능을 의미합니다. OPC는 기존 약어를 따르는 OPC DA (Classic OPC) 가 발전하여 OPC UA (Open Platform Communications unified Architecture : OPC UA) 로 변경되었습니다. Level 2 에서는 현장 설비 각각의 PLC와 통신을 구성하여 현장에서 발생하는 많은 센서 데이터 및 각종 측정 데이터를 수집합니다.하지만 PLC 제조사 마다 다른 통신 프로토콜을 제공하고 있기 때문에 여러 통신 프로토콜을 구성하고 관리하기 어려운 문제들이 발생합니다. 그래서 이러한 문제점을 해결하기 위해 Level 2 에서는 OPC 기술을 사용하는데 OPC는 표준화 된 통신 프로토콜을 제공함으로써 제조사 별로 나누어진 통신 프로토콜을 한 가지 통신으로 구성하여 사용자에게 효율적인 통신 서비스를 제공하게 됩니다. 아래 예시 그림에서 OPC 서버는 각각 다른 PLC 프로토콜과 통신하고 OPC 서버와 OPC 클라이언트에게 PLC 데이터를 상위 시스템에서 사용할 수 있도록 합니다. OPC 참고 사항OPC Classic 에서 제공하는 서비스 DA (Data Access : DA) : PLC의 현재 값을 확인하고 받아올 수 있습니다.→ 과거의 데이터는 받지 못합니다. 이 부분이 HDA와 비교된다고 볼 수 있습니다. HDA (Historical Data Access : HDA) : 과거의 데이터를 가져올 수 있습니다.→ OPC HDA 서버에서 현재 데이터를 Local Historian 에 저장하고 클라이언트 에서 Access 합니다. AE (Alarm &amp; Events : AE) : 알람 &amp; 이벤트입니다.→ TAG를 등록하면 해당 알람이 True로 바뀌었을 때 이벤트로 클라이언트에게 알려주는 기능입니다.\\ Classic OPC (OPC DA) 와 OPC UA 차이 OPC Classic 의 경우 아래 그림 1. 과 같이 서버가 분리되어 있고 그림 2. 와 같이 Windows의 기술인 COM과 DCOM으로 통신을 하기 때문에 Windows 환경에서만 사용이 가능합니다. DCOM : Distributed Component object Model, DCOM 으로 네트워크 컴퓨터 사이에서 컴포넌트 간 통신을 위한 Microsoft기술입니다. 하지만 OPC UA는 기존 OPC Classic의 DA/HDA/AE에 Security를 보완해서 크로스 플랫폼으로 제공합니다. 즉, Windows 환경을 벗어날 수 있습니다. 크로스 플랫폼 : 멀티 플랫폼 이라고도 하며 운영체제, 프로그래밍 언어, 컴퓨터 소프트웨어 등이 여러 종류의 컴퓨터 플랫폼에서 동작할 수 있다는 것을 뜻하는 용어입니다. OPC ServerOPC 표준을 지원하는 산업용 자동화 및 IoT를 위한 연결 플랫폼으로 제조업에서 일반적 널리 사용되는 약 150개 이상의 드라이버를 제공하는 소프트웨어입니다. kepware OPC Server, Takebishi OPC Server, LS OPC Server 등이 있습니다. OPC Server는 산업 제어 시스템 고유의 통신 요구 사항에 맞추었으며 장비를 자동으로 연결하고 장비의 데이터를 읽고 쓸 수 있습니다.또한 해당 데이터를 HMI, SCADA, MES, Historion 또는 ERP 시스템에 완벽하게 통합할 수 있고 높은 신뢰성과 사용자 편의성으로 신속하게 설치하여 완벽하게 작동할 수 있도록 합니다. 우리 Level 2에서는 Kepware OPC Server를 통해 PLC 데이터 및 센서 데이터를 활용하고 있습니다. 2. IPC Shared Memoey (공유 메모리)IPC (Inter Process Communication) 는 프로세스 간 통신이라고 말하며 프로세스 (프로그램) 사이에서 서로 데이터를 주고 받는 방법이라고 생각하시면 됩니다. IPC의 종류 : PIPE, Message Queue, Shared Memory, Memory Map, Socket 아래 그림과 같이 공유 메모리는 프로세스 간 공유가 되도록 설정한 메모리이며 모든 프로세스가 접근이 가능합니다.그리고 이렇게 메모리 자체를 공유함으로써 처리 속도도 빠르다는 장점도 있어 유용하게 사용합니다. 하지만 공유 메모리는 메모리 영역에 대한 동시적인 접근을 제어하기 위한 방법이 필요하여 별도의 동기화 기술이 필요합니다. Level 2 에서는 현장 설비 곳곳에서 실시간으로 변화하는 많은 센서 데이터와 각종 측정 데이터를 수집하여 공유메모리에 적재하여 사용함으로써 수집하면서 발생하는 데이터 읽기, 쓰기, 복사와 같은 오버헤드 발생을 줄이고 하나의 메모리를 공유함으로써 빠른 처리속도를 제공받기위해 공유메모리를 사용하고 있습니다. 3. Message Queue (메시지 큐)MOM을 구현한 시스템을 메시지 큐 ( Message Queue : MQ) 라 합니다. 메시지 큐는 별도의 공정 작업을 연기할 수 있는 유연성을 제공하여 SOA의 개발에 도움을 줄 수 있습니다. MOM : 메시지 지향 미들웨어 (Message Oriented Middleware : MOM)는 비동기 메시지를 사용하는 다른 응용 프로그램 사이에서 데이터 송수신을 의미합니다.&nbsp;SOA : 서비스 지향 아키텍처 Service-Oriendted Architecture : SOA 는 네트워크에서 공통의 통신 언어를 사용하는 서비스 인터페이스를 활용하여 소프트웨어 구성 요소를 다시 사용할 수 있게 만드는 소프트웨어 설계 유형입니다. 그리고 MQ는 비동기, 비동조, 탄력성, 과잉, 보증, 확장성이라는 장점을 가지고 있습니다. 비동기 (Asynchronous) : Queue에 넣기 때문에 나중에 처리 가능하고 다른 API에게 위임함으로써 Request에 대해 빠르게 응답 가능합니다. 비동조 (Decoupling) : MQ를 따로 둠으로써, 애플리케이션과 분리 가능합니다. (결합도를 낮춰 유연성 제공) 탄력성 (Resilience) : MQ는 데이터를 Queue에 담아 비동기로 처리하기 때문에, 일부가 실패 시 전체에 영향을 받지 않음. 과잉 (Redundancy) : 실패할 경우 재실행 가능합니다. 보증 (Guarantees) : 처리된 작업을 확인 가능합니다. 확장성 (Scalable) : 다수의 프로세스들이 큐에 메시지 전송이 가능합니다. Level 2 에서는 아래 예시 그림과 같이 수집한 데이터들을 통해 이벤트 발생기를 구성하였다고 가정하였을때 Message Queue가 없는 경우 프로세스 간 의존성 (결합도)가 높고 기존 동기화 방식은 많은 데이터가 전송될 경우 병목이 생기거나 뒤에 들어오는 요청이 딜레이 되는 등 시스템에 영향을 끼치는 경우가 발생할 수 있습니다. 하지만 Message Queue 를 활용한다면 비동기 처리를 함으로써 프로세스 간 유연성을 높이고 Queue는 별도로 데이터를 보관하고 있기 때문에 프로세스 종료 등과 같은 서비스 장애에 대해 데이터 손실에 대한 부담 또한 줄일 수 있습니다. 이러한 Message Queue의 장점을 Level 2에서 활용하고 있습니다. Message Queue 참고 사항Rabbit MQ MQ의 대표적인 오픈소스이고 AMQP를 따르는 메시지 브로커 소프트웨어 (MOM) 제품 중 하나 입니다. AMQP : Advanced Message Queuing Protocol : AMQP 는 조직 또는 애플리케이션 간의 비즈니스 메시지 통신을 위한 완벽한 지능적 상호 운용성을 제공하는 오픈 소스 표준입니다. ※ AMQP를 구성하는 요소는 Exchange, Queue, Binding이 있습니다. Exchange : 생산자로부터 수신한 메시지를 적절한 큐나 다른 Exchange로 분배하는 라우터의 기능을 합니다. 그리고 Exchange는 데가지 타입으로 바인딩 가능합니다. (Direct, Topic, Headers, Fanout) 타입 설명 Direct Routing key 가 정확히 일치하는 Queue 에 메시지 전송 Topic Routing Key 패턴이 일치하는 Queue에 전송 Headers [Key : Value]로 이루어진 hearder 값을 기준으로 일치하는 Queue에 메시지 전송 Fanout 해당 Exchange에 등록된 모든 Queue에 메시지 전송 Queue : 메시지를 저장하고 소비자에게 저장한 메시지를 전달하는 역할을 합니다. Binding : Exchange와 Queue 와의 관계를 정의한 일종의 라우터 테이블 입니다. (특정 조건에 맞는 메시지를 특정 큐에 전송하도록 설정합니다.) Rabbit MQ 는 메시지를 생산하는 생산자(Producer)가 메시지를 큐에 저장해 두면 Publish/subscribe 방식의 메시지 전달 브로커입니다. 그리고 Rabbit MQ는 대표적인 오픈 소스로써 상업적 지원이 가능하고 AMQP형식을 통해 상호 운용성을 제공 받아 언어, 운영체제 등 형식이 다르더라도 메시지 통신이 가능합니다. 또한 아래 그림과 같이 UI를 통한 시각적인 관리가 가능합니다. 이상으로 4차 산업혁명의 꽃인 스마트 팩토리 (Smart Factory)를 실현하는 Level 2 Process Computer 업무소개와 관련된 기술인 OPC, 공유메모리, 메시지큐에 대한 소개를 마치겠습니다. 감사합니다!","categories":[{"name":"Data","slug":"Data","permalink":"https://vntgcorp.github.io/categories/Data/"},{"name":"Level2","slug":"Data/Level2","permalink":"https://vntgcorp.github.io/categories/Data/Level2/"}],"tags":[]},{"title":"Grafana를 활용한 데이터 가시화","slug":"grafana_df_op","date":"2021-11-30T06:00:00.000Z","updated":"2021-12-01T06:52:13.682Z","comments":true,"path":"grafana_df_op/","link":"","permalink":"https://vntgcorp.github.io/grafana_df_op/","excerpt":"","text":"안녕하세요, VNTG의 Bigdata Engineer 포지션으로 입사한 전준현입니다. Data Engineer는 기본적으로 데이터의 수집 및 관리를 담당합니다. 기업 내에 발생하는 데이터에 대한 기초 공사를 진행한다고 생각하시면 됩니다. 따라서 어떻게 데이터를 수집할 지, 어떠한 방식으로 데이터를 쌓아나갈지, 어떻게 관리할지를 고민하여 회사 내 데이터 흐름을 생성합니다. 하지만 잘 된 공사에서도 누수, 방음 등 이상 현상이 생기듯, 데이터를 수집하는 과정에서도 문제가 발생할 수 있습니다. 이러한 이슈를 해결하여 데이터 흐름을 원할히 하는 것. 이것이 Data Engineer의 역할입니다. Dataforge팀 진행중인 DT Platform 개발에는 크게 수집, 전처리, 저장, 분석, 가시화의 업무로 구성되어 있으며 이 중에 저는 가시화 업무를 담당하였습니다. 현재는 장비로부터 얻은 데이터를 수집하기 위한 소스코드 분석, 데이터 수집 방식 등 전체적인 프로세스를 이해하는 중에 있습니다. 신규 입사자 온보딩 과정으로 이번 첫 포스팅 주제는 Grafana를 활용한 데이터 가시화에 대한 내용으로 이야기 해보려고 합니다. 목차 Grafana는 무엇인가 Grafana Panel - Visualization Grafana Override 기능 Visualization Panels 종류 Alerting 창 생성 및 설정 기능 Rule &amp; Conditions No Data &amp; Error Handling Notifications Alerting Channel 신규 생성하기 Grafana Data Sources 연결 DB에서 Measurement Schema 찾기 Grafana에서 Schema 찾는 Query 작성법 (Tag / Field 값) Query 작성에 필요한 Macros InfluxDB 데이터 소스 별칭 패턴 Dashboard Import &amp; Export 하기 Dashboard Export 하기 Dashboard Import 하기 Data Link 추가하기 Global Variable 기능 생성 Query variable macro Dashboard Playlist Playlist 생성하기 Playlist 표시 방법 Playlist 제어 방법 Annotation 후기 Grafana 참고 Links Grafana는 무엇인가Grafana의 기능을 알기 전에 Grafana는 무엇인지, 주로 어디에서, 어떤 분야에서 활용되고 있는지에 대한 설명부터 시작하겠습니다. 우선 Grafana는 데이터 분석을 실행하기 위한 오픈 소스 솔루션으로 방대한 양의 데이터를 활용한 맞춤형 대시보드를 통해 앱을 모니터링하는 Tool입니다. (▲ 출처 : hansung io) 일반적으로 Plugin이 가능한 Data Source 모델을 가지고 있으며 Graphite, Prometheus, Elasticsearch, OpenTSDB 와 Influx DB와 같은 가장 많이 활용되고 있는 시계열 데이터베이스 등 다양한 Database를 지원합니다. 특히, Influx DB를 활용하여 *시계열 분석 *하는 것. 간단히 말씀드리면 일정 기간 동안 데이터를 받아 분석하고 모니터링하는 데 도움됩니다. 또한, MySQL 및 PostgresSQL 등의 Database를 활용한 Cloud 모니터링을 기본적으로 지원하며 많은 데이터를 단일 대시 보드로 결합할 수 있는 Tool이라 할 수 있습니다. 그러면 Grafana가 어디에서 활용되고 있는지 알아보겠습니다. Grafana는 방대한 양의 데이터를 활용하여 분석과 모니터링에 효과적인 툴인만큼 인프라, 산업용 센서, 자동화, 날씨, FinTech, E-Commerce 등 다양한 분야에서 실제 활용되고 있습니다. 이러한 Grafana을 활용하기 위해서 5가지 주요 기능에 대해 알려드리겠습니다. 기능 특징 Visualization&nbsp;&nbsp;&nbsp;&nbsp; 연결된 데이터로 부터 다양한 형태의 가시화를 지원하고 있습니다. Alert 데이터에 대한 임계치 설정을 통한 모니터링 알림 기능을 통해 문제를 간편하게 해결하고 있습니다. (SNS 연결 및 SMS 전송) Connect 다양한 데이터소스 연결을 지원하고 있습니다 (Elasticsearch, RBD, influxDB 등) Extend Plugin 확장을 통한 강력한 Data 시각화 보여줄 수 있습니다. Dashboard 시각화 구조화를 위한 템플릿 지원으로 타 사용자의 대시보드 템플릿 Import가 가능합니다. 이러한 기능들을 통하여 데이터를 가시화하고, Alert 기능을 통해 실시간으로 문제를 해결할 수 있게 만들어주며, 다양한 데이터소스 연결을 지원하기 때문에 방대한 양의 데이터를 받아올 수 있다는 장점이 있습니다. Grafana Panel - VisualizationGrafana를 활용하여 Data를 가시화하기 위해서는 가장 우선적으로 어떠한 Data를 그래프로 보여줄지, 테이블로 보여줄지 등 어떠한 방식으로 보여줄지 선택해야 합니다. 방금 말씀드린 어떠한 방식으로 보여주는 기능이 Visualization입니다. 하지만, Panel의 Visualization에 따라 Field 탭 또는 Overrides 탭이 보여지지 않는 경우가 존재합니다. 이에 Overrides탭과 Field탭이 가능한 Panel들을 하단의 그림을 통해 보여드리도록 하겠습니다. 참고로 Visualization은 Dashboard에서 진행하는 것이 아닌 Dashboard 안에 있는 Panel에 있는 기능입니다. Visualization 종류 상단의 그림은 Panel에서 표현할 수 있는 전체적인 Visualization입니다. 그 중 Unplugin 이라고 그림 아래에 Error처럼 나와있는 것들은 아직 플러그인을 설치하지 않아서 나타나는 것으로 플러그인만 설치한다면 활용할 수 있는 것들입니다. 다음으로는 좌측의 메뉴에서 Field와 Overrides 탭이 존재하는 Panel들을 보여드리겠습니다. 하단의 그림을 보시면 붉은색 전체 테두리 중 일부만 붉은색 테두리로 나타내었는데 Graph, Time series, Stat, Gauge, Bar gauge, Table 6가지 Visualization만이 Field와 Overrides탭이 존재합니다. 이를 활용한다면 더욱 풍성해 보이는 모니터링을 만드실 수 있습니다. Overrides 기능은 해당 내용이 끝난 다음으로 바로 말씀드리도록 하겠습니다. 붉은색 둥근 테두리로 되어 있는 종류만이 Field와 Overrides 탭 존재 이처럼 Visualization 기능을 활용하여 원하는 Data를 원하는 형태의 시각화로 보여준다면 더욱 풍성한 Dashboard를 완성하실 수 있습니다. 다음으로는 Field의 각 값의 조정할 수 있는 기능인 Override 기능에 대해 알아보도록 하겠습니다. Grafana Override 기능Grafana를 활용하시다보면 Field값 하나 하나를 변경해주어야 할 때가 있습니다. 이런 경우, Grafana의 Override 기능을 통하여 하나 이상의 Field에 대한 설정을 변경할 수 있습니다. 물론, Override 기능에 대한 Field 옵션은 특정 시각화에서 사용할 수 있으며 기존 Field 옵션에 동일하게 적용됩니다. 우측 탭에 있는 Overrides 버튼을 누르면 하단에 Field의 이름, 범위 등을 변경할 수 있도록 선택하는 기능이 있습니다. 선택한 후, 자신이 원하는 Field 또는 Cell display mode, Column Filter 등이 선택 가능하기 때문에 이를 활용하여 자신이 원하는 Data 시각화를 더욱 풍성하게 할 수 있습니다. Fields with name 에 대한 Override 예시 상단의 Overrides된 예시를 통하여 기본적인 표 형태가 아닌 색과 Gauge도 표시할 수 있습니다. 또한, 각 패널마다 지정할 수 있기 때문에 각 패널에 Overrides를 한다면 대시보드를 한 눈에 파악할 수 있게 만들 수 있습니다. 다음으로는 데이터를 시각화하기 위한 Visualization Panels의 종류에 대해 알아보도록 하겠습니다. Visualization Panels 종류Panel의 종류로는 크게 선, 점, 막대 등으로 나타낼 수 있는 Graphs, 커다란 통계를 표시하는 Stats, Gauge형태의 Numbers, Table 및 Log의 정보를 볼 수 있는Misc 등 다양한 종류로 나뉘어져 있습니다. 각 Panel의 종류, 특징, 간단한 소개로 기술해두었습니다. Graphs &amp; Charts 그래프와 차트는 보통 점, 선으로 나타내는 시각화 패널의 종류로 시간 경과에 따른 상태 변화를 확인할 수 있습니다. 종류 및 특징 Time series : 기본 및 기본 그래프 시각화 State timeline : 시간 경과에 따른 상태 변경에 대한 상태 State history : 시간 경과에 따른 주기적 상태 Bar Chart : 모든 범주형 데이터를 시각화 Histogram : 값 분포를 계산하고 막대 차트로 표시 Heatmap Pie chart Stats &amp; numbers 스탯과 넘버는 게이지 형태로 통계를 표시할 때 많이 사용하는 시각화 패널의 종류로 해당 숫자에 따라 상태가 변화되는 것을 확인할 수 있습니다. 종류 및 특징 Stat : 큰 통계 및 옵션 Gauge : 일반 방사형 게이지 Bar gauge : 수평 또는 수직 막대 게이지 Misc 테이블과 로그, 네트워크에 대한 그래프로 새롭게 올라온 Data가 해당 Field값에 쌓이는 것을 확인할 수 있습니다. 종류 및 특징 Table : 기본이자, 유일한 테이블 시각화 Logs : 로그의 기본 시각화 Node Graph : 유향 또는 네트워크에 대한 그래프 Widgets 위젯 형식은 목록이나 Markdown, RSS 피드를 표시하는 시각화 유형입니다. 종류 및 특징 Dashboard List : 대시보드 목록들을 나열 Alert list : 경고 목록을 나열 Text panel : 마크다운 및 html을 표시 가능 News panel : RSS 피드 표시 가능 Alerting 창 생성 및 설정 기능Grafana에서 중요한 기능 중 하나인 Alerting입니다. Alerting을 통하여 System 문제가 발생한 직후 문제를 쉽게 파악할 수 있으며, 실행 가능한 Alerting을 통하여 다양한 Services 중단을 최소화할 수 있습니다. 또한, 각 Panel에서 Alerting을 지정할 수 있기 때문에 문제를 간단히 해결할 수 있습니다. 물론, 전체에 대한 Alerting이 안된다는 점과 초기 설정 시에 하나씩 Alerting을 해야하는 단점이 있지만 초기 설정과 Panel을 잘 작성해둔다면 문제를 좀 더 쉽게 대처해나갈 수 있습니다. 그럼 하단의 그림을 통해 Alerting을 생성하고, Rule과 Conditions를 설정하는 방법에 대해 보여드리겠습니다. 우선 패널의 편집기로 들어가시면 하단에 쿼리 편집기가 보이실텐데 쿼리 편집기 옆에 Alert 이라는 탭을 눌러주시면 상단의 그림처럼 나타납니다. 이 때, Create Alert라는 파란색 버튼을 선택해줍니다. 해당 버튼을 클릭하면 상단의 그림처럼 Alert 설정 창으로 나타납니다.그러면 Alert 설정은 어떻게 해야하는 것인지에 대한 설명을 그림과 함께 보여드리겠습니다.우선 Rule과 Conditions에 대해 설명하겠습니다. Rule &amp; Conditions [Rule]-해당 Alert창의 이름과 몇 분 간격으로 몇 분간 유지되는지의 규칙을 의미합니다. Name: 설명이 포함된 이름을 입력하는 곳입니다. ( 경고 규칙 목록에 이름이 표시되는 곳이라고 생각하시면 됩니다.) Evaluate every: 스케줄러가 경고 규칙을 평가해야 하는 빈도를 지정하는 곳입니다. (평가 간격입니다.) For : 경고 알림이 트리거되기 전에 쿼리가 구성된 임계값을 위반해야 하는 시간을 지정하는 곳입니다. ▶ Rule Example 1Evaluate every : 1m / For : 5m 1분 간격으로 5분동안 해당 알람이 유지되었을 경우, 알람이 울립니다. [Conditions]-현재 존재하는 유일한 조건 유형 Query는 문자, 시간 범위 및 집계 함수를 지정할 수 있는 조건 ▶ Query Conditions Example 1avg() OF query(A, 5m, now) IS ABOVE 46 avg() : 각 계열의 값을 임계값과 비교할 수 있는 값입니다. (다른 집계함수도 활용 가능합니다.) query(A, 5m, now) A : A 메트릭 탭에서 실행할 쿼리를 정의합니다. 5m, now : “5분 전부터 현재 시간까지”를 의미하는 시간 범위를 설정하는 구간입니다. IS ABOVE 46: 임계값의 유형(IS ABOVE)과 임계값(46)이 정해지는 곳입니다. Conditions When 하단의 + 버튼으로는 AND 또는 OR 연산자만 지원합니다. 다음으로는 No Data &amp; Error Handling에 대해 설명드리도록 하겠습니다. No Data &amp; Error Handling 규칙 평가 엔진이 데이터를 반환하지 않거나 null값만 반환하는 쿼리를 처리하는 방법을 구성하는 조건입니다. [If no data or all values are null Option] Alerting 경고 규칙 상태를 Alerting으로 설정합니다. No Data 경고 규칙 상태를 No Data로 설정하는 것으로 No Data일 경우, 알람이 울리게 하는 옵션입니다. Keep Last State 마지막 상태를 유지하는 설정으로 현재 경고 규칙 상태를 유지합니다. Ok 상황이 괜찮은데도 불구하고 그냥 확인하고 싶을 때 설정하시면 됩니다. (확인용) [If execution error or timeout Option] Alerting 상단의 경고 규칙 상태인 Alerting과 동일합니다. Keep Last State 상단의 옵션과 마찬가지로 마지막 상태를 유지하는 설정으로 현재 경고 규칙 상태를 유지합니다. 마지막으로 Alert의 마지막 설정인 Notifications에 대한 설정입니다. Notifications 해당 알람을 보내는 위치와 메모를 설정하는 곳이라고 생각하시면 됩니다. 그럼 설정 하나씩 설명드리겠습니다. Send to : 경보 알림 채널이 설정되어 있는 경우 선택하여 해당 루트로 알람을 보내는 기능입니다. Message : Send to에서 설정한 알림 채널에 어떠한 내용을 포함하여 보낼지 메세지를 작성하는 영역입니다. Tags : 알림에 포함할 Tag(Key/Value) 목록을 지정하는 곳입니다. New tag name, New tag value에 내용 작성 후, 아래에 +Add Tag 를 눌러 Tags를 추가하시면 알람이 울릴 시에 작성하신 Tag Name과 Value가 해당 루트로 보냅니다. Alerting Channel 신규 생성하기Alerting Channel을 생성해야 Notifications의 보내는 Root로 알람을 보낼 수 있기 때문에, Alerting을 생성하기 이전에 우선적으로 진행되어야 합니다. Channel을 생성하지 않은 경우, Alerting이 불가능하기 때문에 자신이 원하는 Root로 지정하는 것을 추천드립니다. Alerting Channel을 생성하는 방법은 하단의 그림과 함께 소개드리겠습니다. 원하는 대시보드의 패널에 들어가 좌측에 있는 종 모양에 마우스를 오버하면 Alerting이라는 메뉴가 나오는데 여기서 두 번째에 있는 Notification channels를 선택합니다. 그럼 해당 창에서 Alerting이라는 창으로 이동하게 됩니다. 이전 단계에서는 Alert Rules 탭에서 설정했다면 이번에는 Notification channels에서 진행합니다. 이 창에서는 파란색 버튼으로 보이는 New Channel을 선택해줍니다. 그러면 New Notification channel로 창이 변경됩니다. 그러면 이제 해당 설정창에서 어떠한 방식으로 작성해야하는지 알려드리겠습니다. 가장 상단에 있는 Name, Type, Addresses 각각을 작성해줍니다. Name : 해당 채널명을 작성하는 곳으로 해당 Name은 모든 패널에서 활용이 가능합니다. Type : Discode, Email, Line 등 다양한 Type들이 존재합니다. 그 중 Email을 가장 선호하며 보통은 Email을 통해 Test 진행하는 것을 추천드립니다. Addresses : Type 선택 후, 해당 이메일 주소를 작성하시면 됩니다. (Type에서 Email을 선택한 경우) 다음은 Type이 Email이기 때문에 이메일을 어떻게 보낼지를 설정하는 Optional Email settings입니다. Single Email 한 번의 알람당 하나의 메일만 보내주는 것입니다. (Addresses 부분에 다수의 주소를 작성해도 한 번밖에 가지 않습니다.) 다음은 Notification 전체에 대한 Setting입니다. Default 이 채널로 전체 알람을 사용할 때 사용합니다. Include image 이메일 내용 안에 알람이 울린 경우의 그라파나 이미지를 보내주어 효과적으로 알람을 보냅니다. Disable Resolve Message 경고 상태가 false로 반환될 때 전송되는 확인 메시지를 비활성화 시킵니다. Send reminders 트리거된 경고에 대한 추가 알림을 보내줍니다. 여기까지 진행하셨으면 Alert 초기 설정이 거의 끝났습니다. 이제 설정이 끝났으니 알람이 제대로 가는지를 확인하기 위해 하단의 Test 버튼을 선택하시면 됩니다. Email인 경우, 네트워크 상황에 따라 메일 받는 시간이 조금 느려질 수 있기 때문에 잠시 기다려줍니다. Test 진행이 완료가 되었다면, Save 버튼을 눌러 채널을 생성해주시면 됩니다. 생성이 끝나셨으면 채널 생성하는 것은 끝이 나게 됩니다. 이제 Alert을 해당 대시보드에 적용해보겠습니다.우선 원하는 대시보드의 패널 편집기로 들어갑니다. Alert Tab 클릭 후 Create Alert 을 선택하고, Notification에 Send to 옆 + 버튼 클릭 시, Channel 에 대한 모든 것이 리스트로 나타나는 것을 볼 수 있습니다. 이후 Message 를 작성하고 Tag 를 설정한 후, 좌측 상단에 있는 &lt;-버튼을 선택하여 저장합니다. 해당 알람의 조건이 맞으면 Grafana에서 해당 메일로 알람 조건 시, 작성한 메세지를 전송합니다. 그럼 다음으로는 Database를 어떻게 연결하고, 원하는 Measurement의 스키마는 어떻게 찾는지, Query는 어떻게 해야하는지 등을 다뤄보겠습니다. Grafana Data Sources 연결Grafana는 Database와 연결함으로써 Table에 있는 Value들을 가져와 활용할 수 있습니다. 그렇다면 Database는 어떻게 연결하는 것인지를 알아보도록 하겠습니다. 공식적으로 지원 가능한 Database Source 종류가 많기 때문에 해당 내용에 대해서는 Data Sources 연결에 대한 내용 이후에 작성되어 있으니 참고해주시면 될 것 같습니다. (출처 : Grafana Document) 우선, 기존에 사용하시던 분들이 아닌 새로 Grafana를 사용하시는 분들이 보실 것이라 생각이 들어 새롭게 생성한 Database를 연결하는 방법에 대해 하단의 그림을 참고하여 설명해드리겠습니다. 원하는 Data Source를 한 번 등록하면,해당 IP로 접속 가능한 모든 User가 사용 가능하니 한 번만 등록해주면 됩니다. 우선 Grafana의 첫 페이지에서 표시된 부분을 마우스 오버하면 나타나는 Data Sources 를 클릭해줍니다. 현재 Grafana와 연결되어 있는 Database가 나열되어 있는데 이 중에 해당 Database가 없다면 상단의 Add data source 버튼을 클릭합니다. 해당 버튼을 누르게 되면 다양한 종류의 Database가 나타나는데, 이 때 원하는 Database를 Search한 후, 선택하시면 됩니다. 예를 들어, Influx DB 를 선택하셨다면 HTTP 부분을 작성하시고 하단으로 내려보면 Save &amp; Test 버튼이 있는데 이 버튼을 누르면 자동 저장되는 것을 확인할 수 있습니다. 이렇게 연결된 Database는 Data Sources안에 자동 저장되어 나열된 것을 확인하실 수 있습니다.Grafana 공식 홈페이지에서 안내하는 지원가능한 데이터 소스 종류는 다음과 같습니다. Alertmanager AWS CloudWatch Azure Monitor Elasticsearch Google Cloud Monitoring Graphite InfluxDB Loki Microsoft SQL Server (MSSQL) MySQL OpenTSDB PostgreSQL Prometheus Jaeger Zipkin Tempo Testdata(출처 : Data sources) DB에서 Measurement Schema 찾기그렇다면 Grafana에서 기존 Database Source는 연결되어 있지만 정확한 Field값을 확인해야 합니다. 보통은 해당 Database를 문서로 정리되어 있거나, Database Tool을 사용하여 Field값을 찾는 방법도 있습니다. 하지만 Grafana 자체 내에서 원하는 measurement에 대한 Schema를 찾는 방법도 있습니다. 그렇다면 어떻게 Schema를 찾는지, Query는 어떻게 작성해야하는지를 보여드리도록 하겠습니다. 우선 Grafana와 연결되어 있는 Database Source를 찾기 위하여 Grafana의 첫 페이지를 들어가시면 우측 사이드 메뉴에 있는 Explore를 선택해줍니다. 그러면 Explore 화면으로 전환되고, Explore 옆에 있는 Database를 드롭다운하면 현재 접속중인 Grafana에 연동중인 Database가 나타납니다. 이 중 원하는 Database를 선택하시면 됩니다. 기존에 연결한 Database가 없다면 드롭다운에 나타나지 않습니다.그런 경우, 바로 윗 내용인 Data Source 연결하기를 참조해주시기 바랍니다. 그후, Query 작성하는 부분에서 왼쪽에 보시면 아주 조그만한 연필 모양으로 되어 있는 Edit 기능이 있는데 이 부분을 눌러 Query 작성 부분을 클릭이 아닌 직접 작성할 수 있는 모양으로 변환하게 됩니다. Query를 직접 작성하셨다면 Format As 라고 보이는 부분 옆에 드롭다운 메뉴를 눌러 Table 형태로 변경합니다. 이후, 좌측 위에 파란색 버튼의 Run query를 누릅니다. 그러면 해당 Query가 작동되면서 하단에 해당 Database에 있는 Schema 정보가 나타나는 것을 확인할 수 있습니다. 현재 사용하고 계시는 Database Tool이 없다면 Grafana에서 직접 찾아보는 방법도 있다는 것을 알려드리고 싶어서 작성해보았습니다. 그렇다면 Grafana에서 Schema를 찾는 Query문 작성 방법과 Query 작성 시에 필요한 매크로가 무엇이 있는지 보여드리겠습니다. Grafana에서 Schema 찾는 Query 작성법 (Tag / Field 값) 원하는 Measurement에서 Tag값 찾는 Query 1Show tag keys from measurement 원하는 Measurement에서 Field값 찾는 Query 1Show Field keys from measurement Query 작성에 필요한 MacrosQuery를 쉽게 작성하기 위해서는 Grafana macros를 활용한다면 더욱 빠르게 Query를 작성할 수 있습니다. 하단의 내용을 참고하셔서 원하는 Query를 작성하시면 됩니다. Macro Results $__time(column) column as “time” $__timeEpoch extract(epoch from column) as “time” $__timeFilter(column) column BETWEEN ‘2017-04-21T05:01:17Z’ AND ‘2017-04-21T05:01:17Z’ $__unixEpochFilter(column) column &gt;= 1492750877 AND column &lt;= 1492750877 $__timeGroup(column,’5m’[, fillvalue]) column &gt;= 1494410783152415214 AND column &lt;= 1494497183142514872 $__timeGroupAlias(column,’5m’) (extract(epoch from column)/300)::bigint*300 AS “time” $__unixEpochGroup(column,’5m’) floor(column/300)*300 $__unixEpochGroupAlias(column,’5m’) floor(column/300)*300 AS “time” InfluxDB 데이터 소스 별칭 패턴InfluxDB 데이터 소스 별칭 패턴을 활용한다면 더 쉽게 Grafana에 적용할 수 있기 때문에 해당 내용을 확인하고 작성하는 것이 좋습니다. 데이터 소스 별칭 패턴에 대해서는 하단에 간단히 설명해두었으므로 참고하여 Grafana에 쉽게 적용하시면 될 것 같습니다. 소스 별칭 설명 $m 측정 이름으로 대체됩니다. $measurement 측정 이름으로 대체됩니다. $col 열 이름으로 대체됩니다. $tag_exampletag&nbsp;&nbsp;&nbsp;&nbsp; exampletag태그 값으로 대체됩니다. ● 구문은 $tag*yourTagName로 시작해야 합니다.&nbsp;&nbsp;&nbsp;&nbsp;ex. $tag● ALIAS BY 필드에서 태그를 별칭으로 사용하려면 태그를 쿼리에서 그룹화하는 데 사용해야 합니다.&nbsp;&nbsp;&nbsp;&nbsp;○ [[tag_hostname]] 패턴 대체 구문으로 사용 가능합니다.&nbsp;&nbsp;&nbsp;&nbsp;○ [예시] - ALIAS BY 필드에서 이 텍스트를 사용하면 각 범례의 값입니다.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;■ Host: [[tag_hostname]]에 대한 hostname태그 값으로 대체 Dashboard Import &amp; Export 하기Grafana 대시보드는 UI 또는 HTTP API 에서 쉽게 Import와 Export 기능이 있어 대시보드의 이동과 복사가 편리하다는 장점이 있습니다. 그러면 대시보드를 Export하고, Import하는 방법에 대해 알아보겠습니다. Dashboard Export 하기Dashboard는 Grafana JSON 형식으로 내보내지며 추후 Dashboard를 가져오는 데 필요한 모든 것( Layout, Variable, Style, DataSource, Query 등)을 포함합니다.Export기능은 대시보드 메뉴에서 Share 버튼을 클릭하여 진행하면 됩니다. 그럼 Dashboard를 Export하는 부분을 보여드리도록 하겠습니다. 우선 원하는 폴더에 원하는 Dashboard를 선택합니다. 그러면 해당 Dashboard로 들어오는데 상단에 Star 아이콘 옆에 Share 아이콘을 선택해줍니다. 상단 Tab에서 Export를 선택해줍니다. 그러면 Save to file, View JSON, Cancel 버튼이 나오게 되는데 이 때 View JSON버튼을 선택해줍니다. 그러면 해당 Dashboard에 나타난 Data들이 JSON형태로 나타나는데 이 때 하단에 Copy to Clipboard 버튼을 선택해줍니다.이렇게 하면 Dashboard Export하는 부분은 끝이 나게 됩니다. 그러면 Export를 진행해보았으니 Import도 같이 진행해보도록 하겠습니다. Dashboard Import 하기대시보드 JSON 파일 업로드 또는 Grafana.com 대시보드 URL을 붙여넣거나 대시보드 JSON 텍스트를 텍스트 영역에 직접 붙여넣기가 가능합니다.따라서, 원하는 Dashboard를 다른 Folder에 데이터 그대로를 가져가고 싶다면 해당 기능을 활용하여 Dashboard를 새롭게 작성해야 한다는 부담을 줄일 수 있습니다. 처음으로 Folder로 돌아가 상단에 파란색 버튼 끝에 있는 Import 버튼을 클릭해줍니다. 그다음 Export할 때, Cpoy to Clipboard 했던 것을 Import via panel json 붙여줍니다. JSON 파일을 붙였다면 하단의 Load를 클릭하면 해당 Dashboard로 Import 되는 것을 확인할 수 있습니다. Data Link 추가하기Data Link를 활용하시면 원하는 Dashboard로 이동이 가능한 Grafana만의 드릴다운 기능을 활용할 수 있습니다. 특히, series name 또는 커서 아래의 값을 포함하는 링크 또한 생성할 수 있기 때문에 원하는 내용을 선택했을 때 원하는 Dashboard URL로 이동할 수 있다는 장점이 있습니다.또한, Link 자체는 시각화에 따라 다른 방식으로 액세스가 가능합니다. 또한, Data Link의 변수를 사용하여 보존된 데이터 필터가 있는 상세한 대시보드는 사람들에게 보낼 수 있습니다. 그렇다면 Data Link를 삽입하는 방법에 대해 보여드리겠습니다. 우선, 원하는 Panel로 들어가서 우측에 탭에 Panel을 선택하시면 아래에 Link 탭이 있는 것을 확인할 수 있습니다. 그 탭을 선택하면 +Add link 라고 나타나는데 이것을 선택합니다. 그러면 상단의 그림처럼 Edit link 라는 하나의 모달창이 뜨게 됩니다. 여기서부터는 순차적으로 해당 링크에 대한 Title을 작성해줍니다. 그리고, 어디로 보낼 것인지에 대한 URL 주소를 넣어줍니다.맨 아래 Open in new tab 에 토글 버튼을 활성화시키시면 해당 URL주소가 작성된 페이지로 새 창이 열리게 됩니다. 이처럼 Data Link를 활용하시면 하나의 대시보드에서 다양한 모니터링을 진행할 수 있습니다. Global Variable 기능 생성Grafana에는 쿼리 편집기의 표현식에서 사용할 수 있는 전역 내장 변수(Global Variable)가 있습니다. 이 항목에서는 알파벳순으로 나열하고 정의하며, 이러한 변수는 Query, Dashboard Link, Panel Link 및 Data Link를 활용할 때 굉장히 유용합니다. 그렇다면 전역 내장 변수(Global Variable)를 어떻게 사용하는지를 보여드리겠습니다. 우선 해당 패널에 들어가 우측 상단의 톱니모양 버튼(해당 패널의 설정)을 클릭합니다. 그러면 설정창으로 변경되면서 좌측 사이드바 메뉴에 있는 Variables를 선택합니다. 그러면 가장자리의 화면이 변경되면서 새롭게 화면이 나타나는데 이 때 가운데 파란색 박스(Add variable)를 선택합니다. 여기까지 진행하셨으면 Global Variable을 편집하는 화면으로 변경된 것을 확인할 수 있습니다. 그럼 해당 Form을 작성하여 Global 변수를 활용할 수 있는데 어떻게 Global Variable을 사용할 수 있는지 알아보겠습니다. 6가지의 다양한 Type이 존재하는데 이 중 Query를 기준으로 작성된 내용입니다. 우선 처음 붉은색으로 표시된 부분은 Variable의 Name, Label, Type, Hide, Description(설명)으로 나뉘어져 있는 것을 볼 수 있습니다. 가장 우선적으로 해야할 것은 Type을 선택하는 것입니다. Type 부분을 선택하지 않고 Name과 Label을 먼저 작성한 경우, Type 선택하면 초기화 되기 때문에 Type을 우선적으로 선택해야 해당 Type에 맞는 Form을 작성할 수 있습니다. Global Variable Type은 선택한 Type에 따라 작성하는 Form이 다릅니다. 가장 많이 사용되는 Type은 Query, Custom, Datasource 로 Query Type은 상단의 그림처럼 작성하시면 됩니다. 쿼리 옵션 부분으로 어떠한 Data source를 활용할 것인지, Refresh는 몇 초 또는 몇 분 간격으로 할 것인지, Query 또는 Regex(정규식)은 어떻게 표현할 것인지를 보여주는 곳입니다. 이는 Type이 Query인 경우에만 작성할 수 있는 Options입니다. 하단의 Query variable은 macro를 참고하여 Query를 작성하시면 됩니다. Query variable macro Name Description Used API endpoints label_names() 레이블 이름 목록을 반환합니다. /api/v1/labels label_values(label) label모든 메트릭 의 레이블 값 목록을 반환합니다. /api/v1/label/ label/값 label_values(metric, label) label지정된 메트릭 의 레이블 값 목록을 반환합니다. /api/v1/시리즈 metrics(metric) 지정된 metric정규식과 일치하는 메트릭 목록을 반환합니다. /api/v1/label/이름/값 query_result(query) query에 대한 Prometheus 쿼리 결과 목록을 반환합니다. /api/v1/쿼리 Query에서 작성된 내용에 대한 Options입니다. Multi-value의 toggle button을 On 으로 설정한 경우, Query에서 작성한 Field를 복수로 선택 가능합니다. 그 하단에 Include All Option 을 On 으로 설정한 경우, Field에 대해 All이라는 것이 생겨 전체를 선택하고 싶다면 설정해두는 것이 좋습니다. 마지막으로 Preview of values 부분은 위에서 작성하고 설정한 모든 값이 실시간으로 나타나는 부분입니다. General, Query Options, Selection Options 등 모든 것을 확인한 후, 하단에 Update 버튼을 클릭하면 해당 Dashboard의 Global Variable가 설정됩니다. Dashboard PlaylistPlaylist는 순서대로 있는 대시보드 목록을 표시하는 것으로 재생 목록 Interval생성 또는 편집 시 설정 한 필드에 지정된 시간 동안 각 대시보드를 표시하는 기능입니다. 그럼 Playlist는 어떻게 생성되는지, 표시 방법은 무엇인지, 제어방법은 무엇인지에 대해 알아보겠습니다. Playlist 생성하기 Grafana 첫 페이지에 들어가서 메뉴 좌측 사이드바에 있는 Playlist를 선택합니다. playlist를 선택하면 가운데 창이 Dashboards로 변화하면서 Playlist가 표시됩니다. 그 좌측 상단에 있는 `New playlist`를 선택해줍니다. 그러면 상단의 New Playlist라고 표시되며 창이 바뀌는데 이 때, 붉은색으로 표시된 Playlist의 Name과 몇 분 간격으로 보여줄 것인지에 대한 Interval값을 입력하시면 됩니다. 그 후에 Playlist에 담을 Dashboard를 선택하여 줍니다. (여러 개의 Dashboard를 추가할 수 있습니다) Add to playlist를 선택하시면 상단의 그림처럼 Dashboards가 추가되고 하단의 Create를 선택하면 해당 Dashboards가 Playlist에 담겨져 있는 것을 볼 수 있습니다. Playlist 표시 방법상단의 playlist를 생성했다면 playlist를 표시하는 방법에 대해 설명드리겠습니다. 생성한 playlist에서 Start playlist를 클릭하시면 5가지 표시 방법을 나타내는 방법이 있는데 그 내용은 하단의 내용을 참고하여 원하시는 표시 방법대로 표시하면 됩니다. 방법 설명 일반 모드(Normal mode) ● 사이드 메뉴는 계속 표시● 탐색 모음, 행 및 패널 컨트롤이 화면 상단 표시 TV 모드(TV mode) ● 사이드 메뉴 및 대시보드 하위 메뉴(변수 드롭다운 및 대시보드 링크 포함)가 숨겨지거나 제거● 탐색 모음, 행 및 패널 컨트롤이 화면 상단 표시● 사용자가 1분 동안 활동이 없으면 자동으로 활성화● d v시퀀스 바로 가기를 사용하거나 ?inactive대시보드 URL에 매개변수 를 추가 하여 수동으로 활성화 TV 모드(자동 맞춤 패널 포함) ● 사이드 메뉴 및 대시보드 하위 메뉴(변수 드롭다운 및 대시보드 링크 포함)가 숨겨지거나 제거● 탐색 모음, 행 및 패널 컨트롤이 화면 표시● 대시보드 패널은 화면 공간을 최적화하기 위해 자동으로 조정 키오스크 모드(Kiosk mode) ● 사이드 메뉴, 탐색 모음, ro 및 패널 컨트롤은 보기에서 완전히 숨겨지거나 제거● d v 재생 목록이 시작된 후 시퀀스 바로 가기를 사용하여 수동으로 활성화 가능● 동일한 바로 가기를 사용하여 수동으로 비활성화 가능 키오스크 모드(자동 맞춤 패널 포함)&nbsp;&nbsp; ● 사이드 메뉴, 탐색 모음, 행 및 패널 컨트롤이 완전히 숨겨지거나 보기에서 제거● 대시보드 패널은 화면 공간을 최적화하기 위해 자동으로 조정 Playlist 제어 방법해당 설정을 모두 끝낸 후 playlist를 시작하면 화면 상단의 탐색 모음을 사용하여 제어 할 수 있습니다. 단추 결과 다음(이중 오른쪽 화살표)&nbsp;&nbsp; 다음 대시보드로 이동 뒤로(왼쪽 화살표) 이전 대시보드로 이동 정지(사각형) 재생 목록을 종료하고 현재 대시보드로 종료 주기 보기 모드(모니터 아이콘) 다양한 보기 모드에서 대시보드 표시를 회전 시간 범위 시간 범위 내의 데이터 표시(아래쪽 화살표를 사용하여 최대 5년 전의 최근 5분 또는 사용자 지정 시간 범위를 표시하도록 설정 가능) 새로 고침(원 화살표) 대시보드를 다시 로드하여 현재 데이터 표시(드롭다운을 사용하여 5초에서 1일마다 자동으로 다시 로드하도록 설정 가능) AnnotationAnnotations은 풍부한 이벤트로 그래프의 포인트를 표시하는 방법입니다. 주석 위로 마우스를 가져가면 이벤트 설명과 이벤트 태그를 볼 수 있으며, 텍스트 필드에는 더 자세한 다른 시스템에 대한 링크를 포함시킬 수 있습니다. Grafana 첫 페이지에서 원하는 Dashboard로 들어가 우측 상단의 설정 버튼을 눌러눕니다. 좌측 사이드바 메뉴에서 Annontations를 선택하여 줍니다. 가운데 파란색의 Add Annotation Query 버튼을 선택해줍니다. Data Source를 기준으로 Annotations이 변경될 수 있기 때문에 Data Source부분을 먼저 선택하여 줍니다. 그 후, 원하는 Query를 작성해 줍니다. Query 내용까지 모두 작성하였으면 하단에 Add 버튼을 눌러 Annotations를 추가시켜 줍니다. 이렇게 추가된 Annotations는 해당 Dashboard의 전역으로 사용할 수 있습니다. Data source 부분에 DB를 선택 시, Query 방식으로 작성하는 것이 좋습니다. 후기지금까지 Grafana를 직접 사용하면서 메모했던 기초 설정, Data Source연결 등 다양한 기능들을 알아봤는데, 제가 느낀 Grafana의 최대 장점은 클릭 몇 번 또는 간단한 쿼리문으로도 쉽게 모니터링 기능을 사용할 수 있다는 것이었습니다. 특히, 대용량의 데이터 중 톡톡 튀는 데이터를 확인하고 싶은 상황에서 Grafana에서 무료로 제공되는 Alert 기능을 활용하기에 좋을 것 같습니다!! 다음 포스팅에서는 좀 더 간결하고 재미있는 주제로 작성해보겠습니다! 지금까지 긴 글 읽어주셔서 감사합니다!! Grafana 참고 Links Grafana 도움 검색 페이지 Grafana Help Page Grafana 샘플 페이지(Ver 8.1.2) Grafana Sample Page Grafana Community Grafana 커뮤니티 Cloud Z 모니터링 사용자 가이드(Grafana) Cloud Z 모니터링 사용자 가이드(Grafana)","categories":[{"name":"Data","slug":"Data","permalink":"https://vntgcorp.github.io/categories/Data/"},{"name":"Grafana","slug":"Data/Grafana","permalink":"https://vntgcorp.github.io/categories/Data/Grafana/"}],"tags":[]},{"title":"Terraform Cloud 기반으로 인프라 구축 협업하기","slug":"devops_md_op","date":"2021-11-29T06:00:00.000Z","updated":"2021-11-30T08:59:31.777Z","comments":true,"path":"devops_md_op/","link":"","permalink":"https://vntgcorp.github.io/devops_md_op/","excerpt":"","text":"MD@Technology Management안녕하세요. VNTG에 DevOps Engineer 포지션으로 입사한 이동열입니다. DevOps Engineer라고 하면 개발과 운영을 하나로 통합해 변화에 빠르게 대응할 수 있도록 도구들을 사용하거나 방법론을 선택해 개발과 운영에 대한 인프라를 구축해나가는 업무를 수행합니다. 저는 회사 체계를 정립하고 다른 조직이 DevOps를 실현할 수 있도록 인프라단위의 지원업무를 맡고 있습니다! 첫 기술블로그 포스팅으로 Terraform Cloud를 활용한 GCP 기반 인프라 협업에 대한 내용을 적어보려합니다. Terraform과 Terraform CloudTerraform은 인프라를 안전하고 효율적으로 구축, 변경 및 버전 관리하기 위한 도구입니다. 클라우드를 사용해 인프라를 구축해 보았다면 일정 규모 이상의 인프라 구축 시 자신이 어떤 리소스를 어떻게 생성 했는지 헷갈릴 때가 있었을 것입니다. 테스트를 위해 지우고 다시 생성하거나, 개발환경에서 Production 환경으로 마이그레이션 할 때, 규모가 크다면 일관성 있는 인프라를 구축하기가 힘들고 어플리케이션이 정상 동작하지 않을 수도 있습니다. 이러한 것들을 예방하기 위해 인프라를 코드로 관리하여 쉽고 간편하게 인프라의 일관성을 유지하고 관리 할 수 있습니다. 이러한 관리 기법을 코드형 인프라(Infrastructure as Code, IaC)라고 하며 Terraform은 IaC에서 대표적으로 사용되는 도구 중 하나입니다. 그럼 Terraform Cloud는 무엇일까요? 인프라를 구축하는 여러명의 Engineer가 Terraform을 사용해 협업한다고 생각해보면 각자가 수정한 부분에 대한 인프라를 모두가 검증할 필요가 있습니다. 특히 인프라의 경우는 시스템 전체에 영향을 끼칠 수 있고 문제 발생시 그 원인을 찾는 것이 굉장히 힘들기 때문에 사전에 철저하게 검토해야합니다. Terraform Cloud를 사용하면 버전관리 시스템과 연동해 코드를 관리하고 인프라가 변경되기전에 모두의 검토를 받을 수 있습니다. 즉, Terraform Cloud는 Terraform으로 쉽게 협업을 할 수 있도록 도와주는 도구입니다. Terraform Cloud 시작하기샘플 Terraform Code를 Fork합니다. GitHub - vntg-ldy/vntg-terrafrom-cloud-gcp-sample 위 Repository의 Terraform 코드는 네트워크 모듈로만 구성되어 있습니다. main branch는 초기 상태의 코드이며, chapter01 branch는 이 블로그에서 진행된 내용이 포함되어 있는 코드입니다. README.md에 적혀있는 대로 따라하시게 되면 web, was, proxy에 대한 네트워크 리소스가 생성됩니다. 그럼 위 샘플 코드를 사용해 Terraform Cloud를 사용해보도록 하겠습니다. 먼저, Terraform Cloud를 사용하기 위해서는 Terraform Cloud 계정을 만들고 작업공간을 생성해야합니다. 작업공간은 Github의 Repository와 같은 역할을 한다고 생각하시면 됩니다. 기본적으로 Terraform Cloud의 작업 공간에는 구성 파일, 환경 변수, 입력 변수, 상태 파일 등이 포함되어 있습니다. https://app.terraform.io/signup/account 위 링크로 이동해 Terraform Cloud 계정을 생성하고 작업 공간을 생성합니다. 계정을 생성하게 되면 이메일 인증 후 setup workflow를 선택하라는 페이지가 출력되는데 그 중 Start from scratch을 선택합니다. 선택을 하게 되면 조직을 생성하는 페이지가 출력됩니다. 원하는 이름으로 조직까지 생성하면 이제 조직내에 작업 공간을 생성할 수 있게 됩니다. 그럼 생성한 조직으로 이동해 작업 공간을 생성합니다. 작업 공간은 Workspaces 우측 상단에 New workspace를 클릭해서 생성할 수 있습니다. 클릭하면 Version control workflow, CLI-driven workflow, API-driven workflow 중에서 하나의 workflow를 선택하는 페이지가 출력되는데, 지금은 Github와 연동해서 사용할 것이기 때문에 Version control workflow를 선택하도록 합니다. 그러면 Provider를 선택하는 페이지가 출력됩니다. 저의 경우에는 이미 이전에 Github와 연동해서 사용한 이력이 존재해 페이지에 Github만 표시되어 있지만 Terraform Cloud를 처음 사용하시는 분들이라면 Github 외에도 여러 Provider들이 표시되어 있을 겁니다. 이 중에 Github를 선택하시면 되며, 이전에 다른 Provider를 사용하셨던 분들이라면 하단의 Connect to a different VCS를 클릭해 Github를 선택할 수 있습니다. Github 버튼을 클릭하게 되면 Github Repository를 선택할 수 있습니다. 하지만 해당 Provider로 처음 생성하는 것이라면 계정 연결 절차를 먼저 진행하게 되며 특별한 것 없이 안내해주는 대로 인증해주시면 됩니다. Repository를 선택했으면 작업 공간의 이름과 설명을 입력해줍니다. Workspaces에 작업 공간이 생성된 것을 볼 수 있습니다. 해당 작업 공간으로 이동하면 Github Repository가 연동돼 README.md가 출력되는 것을 볼 수 있습니다. Terraform Cloud 변수Terraform을 개인적으로 사용해보셨다면 terraform apply 명령어를 사용해 Terraform 코드를 직접 실행 하셨을 것입니다. 하지만 Terraform Cloud를 사용하면 Terraform 코드는 Terraform Cloud에서 실행하게 됩니다. 그 말은 Terraform 코드를 실행시키기 위한 GCP 인증 정보나 실행 시 필요한 각종 변수들이 Terraform Cloud 내에 정의되어 있어야 한다는 뜻입니다. 그럼 제일 먼저 GCP 리소스를 생성하기 위한 GCP 인증 정보에 대한 변수를 생성합니다. 인증 정보에 대한 변수를 생성하기 위해서는 GCP의 인증 정보를 먼저 가지고 있어야 합니다. GCP의 인증 정보는 GCP내 IAM 및 관리자 → 서비스 계정에서 생성할 수 있으며, 서비스 계정 생성 후 해당 계정의 키 관리 메뉴로 이동하면 json 형식의 비공개 키 파일을 만들고 다운로드 받을 수 있습니다. 서비스 계정을 사용해 GCP 리소스를 생성하기 위해서는 서비스 계정이 해당 리소스 생성에 대한 권한을 가지고 있어야합니다. 위 절차대로 GCP에서 비공개 키 파일을 만들고 다운로드 받았다면 Terraform Cloud의 작업 공간으로 이동해 탭 메뉴에서 Variables를 클릭 합니다. 그러면 변수를 관리하는 페이지가 출력됩니다. 이 페이지에서는 작업 공간에서 사용할 수 있는 Terraform 변수나 환경 변수 등을 관리할 수 있습니다. 여기서 환경 변수를 등록하기 위해서는 페이지 중간에 Workspace variables의 Add variable을 클릭합니다. 그 다음 Environment variable(환경 변수)를 체크하고 Key에 GOOGLE_CREDENTIALS, Value에는 위에서 생성한 비공개 키 파일 내용 전체를 복사해 붙여넣습니다. 그리고 노출되면 안되는 민감한 변수 이므로 Sensitive를 체크합니다. Sensitive를 체크하게 되면 헤딩 변수가 UI나 API에 표시되지 않습니다. GCP 서비스 계정의 비공개 키를 확인해보면 줄바꿈이 포함되어 있는 json형식으로 되어 있습니다. Terraform Cloud는 줄바꿈이 포함되어 있는 변수를 등록하면 정상 작동하지 않아 반드시 줄바꿈을 전부 제거한 후 등록해주셔야 합니다. 환경변수를 잘 등록했다면 이제 Terraform 변수도 등록합니다. Terraform 변수는 말그대로 Terraform에서 사용할 변수를 말합니다. 로컬에서 실행했을 때 지정하던 변수값을 똑같이 지정해주면 됩니다. 위에서 받은 코드는 대부분의 변수에 default값이 설정되어 있지만 Project와 Region은 설정되어 있지 않아 변수를 꼭 등록해주어야 합니다. 변수는 아래처럼 추가 해주시면 됩니다. 그럼 이제 Terraform Cloud를 사용해 Terraform Code를 실행시켜봅시다. 페이지 우측 상단에 보면 Actions 버튼을 볼 수 있습니다. 클릭 후 Start new plan을 클릭합니다. 그러면 plan을 시작하는 이유를 적고 최종 실행하게 됩니다. 실행과정은 로컬에서 실행하는 것과 똑같습니다. 무엇을 만들지 보여주고(plan) 팀원들이 승인하면 실제로 배포됩니다. 다른 점이라면 확인과정에서 Comment를 남길 수 있습니다. 모두가 승인했다면 위 plan대로 반영됩니다. 하지만 매번 이렇게 수동으로 코드를 실행시키기는 너무 불편합니다. 어차피 VCS에서 코드 병합전에 검토를 할 것이기 때문에 병합이 된다면 자동으로 코드를 실행하게 하는 것이 좋을 것 같습니다. Terraform Cloud에서 코드를 자동으로 실행시키려면 Automatic speculative plans를 활성화 해주어야합니다. Settings → Version Control로 이동해 Automatic speculative plans를 활성화 합니다. 이제 정상적으로 실행되는지 확인하기 위해 루트 디렉토리의 variables.tf를 열어 vpc_name의 defualt값을 tc-vpc-2로 변경 후 Commit&amp;Push 합니다. 그러면 Terraform Cloud에서 자동으로 실행하는 것을 볼 수 있습니다. VPC 이름을 변경해 Subnetwork까지 함께 Replace된 것을 볼 수 있습니다. GCP에서 네트워크 리소스가 생성된 모습 후기Terraform Cloud를 사용해보면서 느꼈던 가장 큰 장점은 굉장히 조심스럽게 다뤄야하는 인프라를 안전하게 협업할 수 있다는 점과 둘 다 Hashicorp사에서 제공하는 서비스이기 때문에 다른 CI/CD처럼 복잡한 설정없이 쉽게 설정이 가능하다는 점 이였습니다. 하지만 Terraform Cloud는 조직 내 5인이상 사용할 경우 유료이기 때문에 비용에 민감한 조직이라면 무작정 도입하는 것 보다는 5인 이하는 무료라는 점을 이용해 충분히 테스트 해보고 신중하게 결정하시는 것이 좋겠습니다. 이렇게 Terraform Cloud를 직접 사용해보고 블로그를 작성해 보았는데 막상 다 쓰고 보니 너무 간단하게만 다룬 것 같아 아쉬움이 너무 많네요! 하지만 앞으로 VNTG에서 있으면서 Terraform Cloud뿐만 아니라 다른 다양한 기술들과 도구들도 깊게 사용해 볼 것이기 때문에 다음에는 보다 알차고 재미있는 내용으로 준비해서 다시 작성해보도록 하겠습니다! 감사합니다!","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://vntgcorp.github.io/categories/DevOps/"},{"name":"Terraform","slug":"DevOps/Terraform","permalink":"https://vntgcorp.github.io/categories/DevOps/Terraform/"}],"tags":[]},{"title":"Docker Desktop 없이 Windows에서 Docker 사용하기","slug":"DockerWithoutDesktop","date":"2021-09-07T07:00:00.000Z","updated":"2021-12-01T06:50:26.742Z","comments":true,"path":"DockerWithoutDesktop/","link":"","permalink":"https://vntgcorp.github.io/DockerWithoutDesktop/","excerpt":"","text":"찰리브라운@기술지원실지난 8월31일 Docker Desktop 의 상업적 이용에 대한 유료화가 발표되었습니다. 250인 이하 사업장 and 연매출 1,000만달러 이하의 조건을 만족할경우 계속 무료로 사용가능하긴 하지만, 해당하지 않는다면 내년 1월까지의 유예기간 안에 라이선스 구매나 Docker Desktop을 삭제하고 Podman 같은 대체제를 사용한다던지의 선택이 필요해졌습니다. Windows/MacOS 사용자라면 거의 Docker Desktop을 사용했기에 많은 개발자들에게 영향이 있을것 같네요. https://docs.docker.com/get-docker/공식 문서에서 Windows/Mac의 설치방법은 Docker Desktop만 안내됩니다. 앞으로 이 유료화 정책을 각 회사들이 어떻게 회피하거나 받아들이지 궁금합니다만, 오늘은 Windows에 Docker Desktop없이 Docker를 설치해 사용하는 방법을 알아보도록 하겠습니다. Docker Desktop을 사용하는게 편리성에서 최선의 방법이라는 사실은 변함없습니다.WSL2의 사전 설치가 필요합니다. 이하는 Ubuntu 20.04에서 진행했습니다.복수의 WSL을 사용하는 경우 추가적인 설정이 필요합니다. 이번글에서는 다루지 않습니다. WSL을 실행합니다. 검색이나 실행으로 쉽게 찾을 수 있습니다. 이하의 계정 추가는 Microsoft Store를 통해 리눅스를 설치했을때 자동생성되는 계정을 사용할 경우 스킵 가능합니다. 계정 추가 docker를 사용할 계정을 추가합니다.1adduser (username) 계정에 권한 부여 생성한 계정에서 sudo 를 가능하게 합니다.1usermod -aG sudo (username) 사용하는 리눅스 배포판에 따라 명령어는 다를 수 있습니다. 계정을 docker group에 추가 dockerd와 docker는 socket통신을 하기 때문에 권한이 필요합니다.1sudo addgroup (username) docker 생성한 계정을 default 계정으로 설정 계정ID 확인1id -u docuser 예제의 ID는 1001 로 확인됩니다. WSL의 리눅스 이미지 확인 (Powershell 에서 실행)1wsl -l 예제의 리눅스 이미지는 Ubuntu 로 확인됩니다. 확인한 정보를 이용해 WSL 기본 계정 지정 (Powershell 에서 실행)1Get-ItemProperty Registry::HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Lxss\\*\\ DistributionName | Where-Object -Property DistributionName -eq Ubuntu | Set-ItemProperty -Name DefaultUid -Value 1001 설정반영을 위해 WSL을 재부팅 (Powershell 에서 실행)1Restart-Service LxssManager Docker 설치준비 리눅스 배포판별 설치가이드: https://docs.docker.com/engine/install/#server여기서는 Ubuntu를 기준으로 합니다. 기존 Docker 삭제123sudo apt-get remove docker docker-engine docker.io containerd runcsudo apt-get purge docker-ce docker-ce-cli containerd.iosudo rm -fr /var/lib/containerd/ 필요 패키지 설치12sudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-releasesudo apt-get update Docker 공식 GPG 키1curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Docker stable repo 사용1sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; Docker 설치 123sudo apt install docker-ce docker-ce-cli containerd.iosudo curl -L &quot;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-compose Docker daemon(dockerd) 실행 1sudo dockerd 스크린샷과 같은 메시지가 표시되면 dockerd가 실행된겁니다. WSL 터미널을 하나 더 열어 잘 실행되는지 확인합니다. 1docker run hello-world Powershell에서도 실행해봅니다. 1wsl docker run hello-world Docker-compose 실행 Powershell에서 docker-compose도 문제없습니다. WSL에서도 물론 가능합니다.","categories":[{"name":"Docker","slug":"Docker","permalink":"https://vntgcorp.github.io/categories/Docker/"}],"tags":[]},{"title":"FastAPI, WebSocket, 그리고 Docker","slug":"fastapi","date":"2021-06-30T07:05:46.000Z","updated":"2021-11-29T03:57:02.415Z","comments":true,"path":"fastapi/","link":"","permalink":"https://vntgcorp.github.io/fastapi/","excerpt":"","text":"까를로스@기술지원실Intro최근(이라고 하기에는 좀 시간이 흘렀지만) python framework계에는 다양한 프레임워크들이 속속 시장 진입을 하는 것 같다. 그중에서도 가장 주목을 받는 프레임워크는 아마도 FastAPI 일 것이다. 이름부터가 빠르다 + API 이니…이름 만으로도 벌써 반은 먹고 들어가는 기분이다. FastAPI는 Sebastián Ramírez라는 개발자(혹은 그의 팀?)에 의해 만들어 진 듯 하다. 솔직히 자세히 안 파봤다. 쏴리. 아래 사진은 Medium에 있는 그의 프로필 이미지. Sebastián Ramírez 프로필 구글, 유튜브 등을 검색해도 꽤 많은 자료가 나오니 참조하시기 바란다.(내용도 꽤 충실하고 볼만하다) 일단 특이한 수염(살바도르 달리를 닮았다)의 이 개발자가 만든 FastAPI는 다음과 같은 특징을 갖는다: 빠르다(Starlette과 Pydantic 덕분에) 사용가능한 가장 빠른 파이썬 프레임워크 중 하나라고 자신있게 기재할 정도. 빠른 코드 작성: 약 200~300% 개발 속도 증가라고 했는데 실제로도 꽤 적은 양의 코드로도 개발이 가능한 편. 적은 버그: 개발자에 의한 에러 약 40% 감소…인데 내부 개발팀의 테스트에 근거했다고 함. 직관적: 훌륭한 편집기 지원. 자동완성 지원. 적은 디버깅 시간. 실제로도 VSC(Visual Source Code)나 PyCharm과 함께 잘 동작한다. 쉬움: 문서화가 꽤 잘 되어 있다. 심지어 한글(전체는 아니지만)도 존재!(이것도 한글화 문서를 보고 정리하는 중!) 짦음: 코드 중복 최소화. 각 매개변수 선언의 여러 기능. 견고함: production-ready 표준 기반: 이게 가장 중요한 특징이라고 보여지는데, API 개방형 표준을 지원한다! OpenAPI(former Swagger) 및 JSON schema 지원. FastAPI official DocumentFastAPI는 파이썬 3.6+의 API를 빌드하기 위한 modern, fast(performance), standard type hint에 기초한 웹 프레임워크이다. 파이썬 프레임워크 중 최강자라 할 수 있는 django(제발 ‘장고’라고 읽어줘. 재즈 뮤지션 이름이기도 하니깐)나 미니멀 웹 프레임워크인 flask만 알아주던 파이썬 시장에서 새로운 강자로 발돋움하는 중이기도 하다. FastAPI는 Starlette, Pydantic을 통합한 프레임워크라고 두 프레임워크의 장점을 갖고 있고, OpenAPI나 JSON을 편하게 다룰 수 있다는 점 만으로도 강점을 갖는다고 봐. 더 많은 장점은 official document를 찬찬히 살펴보면 좋을 듯. 나중에 Pydantic, Starlette, Uvicorn/Hypercorn 등도 좀 더 자세히 다뤄보고 싶은 주제이기도 하다. 그래서… FastAPI를 느껴 보자!정말 간단한 예제를 통해서 FastAPI를 알아보도록 하자. 1234567from fastapi import FastAPIapp = FastAPI()@app.get(&quot;/&quot;)async def root(): return &#123;&quot;message&quot;: &quot;Hello World&quot;&#125; main.py를 만들고 위 내용을 복사한다. Virtual Environment(venv)에서 먼저 12pip install fastapipip install uvicorn (or hypercorn) 으로 설치를 해두고 라이브 서버를 실행한다. (uvicorn 말고 hypercorn을 써도 되는데, 둘다 ASGi를 지원하는 좋은 서버들이다. 차이가 있다면 hypercorn은 HTTP/1, HTTP2, WebSocket over H/1 and H/2, ASGI/2, ASGI/3를 모두 지원해서 개인적으로 좀 더 선호하고 있다) 1uvicorn main:app --reload 실행하면 터미널에 1INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) 이런 메시지가 보이는데 이 라인은 로컬에서 앱이 서비스되는 URL을 보여준다. 이 URL을 마우스로 클릭하거나 브라우저로 복사해서 오픈하면 아래와 같은 JSON 응답을 확인할 수 있다: 1&#123;&quot;message&quot;: &quot;Hello World&quot;&#125; 여기까지만 보면 뭐가 크게 다른지 안 보인다. 그래서 준비했다! 1http://127.0.0.1:8000/docs 그리고 또 1http://127.0.0.1:8000/redoc 크으..그렇다. 이 두가지 API UI 때문에라도 이걸 써봐야 한다. 그렇지만 이걸로만 끝내면 아까 FastAPI의 장점에서 이야기한 OpenAPI가 슬퍼한다. 1http://127.0.0.1:8000/openapi.json 12345678910111213141516171819&#123; &quot;openapi&quot;: &quot;3.0.2&quot;, &quot;info&quot;: &#123; &quot;title&quot;: &quot;FastAPI&quot;, &quot;version&quot;: &quot;0.1.0&quot; &#125;, &quot;paths&quot;: &#123; &quot;/items/&quot;: &#123; &quot;get&quot;: &#123; &quot;responses&quot;: &#123; &quot;200&quot;: &#123; &quot;description&quot;: &quot;Successful Response&quot;, &quot;content&quot;: &#123; &quot;application/json&quot;: &#123;... 여기까지와 실제 API간의 인터랙션을 위한 기본 학습 내용은 아래 URL을 참조하길 바란다. 문서가 매우 잘 되어 있어서 자습하기에도 좋다. 음 설마 API를 하려고 하니 GET, PUT, DELETE, UPDATE는 … 당연히 알거라 생각해서 패스한다. 첫걸음 - FastAPI실제 제목이었던 내용인 FastAPI, WebSocket, 그리고 Docker에 대한 내용을 이제 알아보….도록 합니다. 다시 FastAPI, WebSocket, Docker순서를 정해서 차례대로 달려 보자. 여기에 있는 예제는 솔직하게, FastAPI Official Document만 잘 찾아봐도 다 할 수 있는 내용이다: FastAPI에 WebSocket Client를 추가해 보자. Dockerize를 해보면 끝. 사실 FastAPI, Docker, WebSocket 각각만 해도 엄청난 양의 블로그, 문서를 만들 수 있을 만큼의 내용이지만 날림의 대가로서 일단 이것만 하고 여유가 되면(=누군가 조공을 바치면?) 각각에 대해서도 썰을 풀어볼까 한다. 1. FastAPI에 WebSocket Client를 추가해 보자.애용하는 IDE에서 project/folder를 만들고 venv를 구성하고, 위에 설명이 있는 것처럼 fastapi, uvicorn을 설치한다. 그리고 보편적으로 사용할 main.py를 만든다. (개인적으로 Golang 은 VSC에서, Python은 PyCharm, 그리고 linux/Mac OS 터미널에서는 nano 에디터를 쓴다. 개인취향이다 ㅠㅠ) 이 main.py를 채우기 위해서 우선 공식문서에서 websocket을 찾아보자. 주욱 읽어가다 보면 WebSocket client에 대한 내용을 볼 수 있다. WebSockets - FastAPI Await for messages and send messages 에 있는 샘플 소스코드를 복사한 후, 에디터의 main.py에 붙여넣기한다. WebSockets - FastAPI 그럼 자신있게 라이브 서버를 실행시켜 보자. 1unicorn main:app --reload 그리고 브라우저를 열고 http://127.0.0.1:8000 을 보면 아래와 같은 페이지를 보게 될 것이다: 오, 됐어! 이렇게 쉬울 수가! 하면서 텍스트 입력 창에 여러가지 써보고 Send를 눌러보면…. 음…아무런 동작을 하지 않는다.. 에디터 터미널 등을 살펴보면 다음과 같은 에러 메시지를 찾아볼 수 있을 것이다: 메시지를 보면, 12WARNING: Unsupported upgrade request.WARNING: No supported WebSocket library detected. Please use &#x27;pip install uvicorn[standard]&#x27;, or install &#x27;websockets&#x27; or &#x27;wsproto&#x27; manually. 아하! WebSocket 라이브러리가 설치 안되었구나. 바로 ‘pip install websockets’로 설치해 주시고 나서 라이브서버 재실행, 그리고 나서 브라우저를 리로드해주고, 텍스트 입력창에 글을 쓰고 send를 누르면, WebSocket이 이렇게 쉬운건가 하는 가벼운 착각(?)을 하면서 아래 내용을 좀 더 읽어 보시길 권함. 사실 async/await 등 개념부터 제대로 이해하는 것이 맞지만 우선 되는 걸 확인하면서 하나씩 짚어 보는 방법도 나쁘지 않다고 보기에, 예제를 열심히 따라하거나 내용을 상세히 읽어 보시길 권합니다~ 2. Dockerize를 해보면 끝!일단 Docker, Container 라고 이야기하면 경기를 일으키는 분들도 꽤 있다(정말이다. 내 주변에도 꽤 있다.) 개인적인 경험으로는 Docker나 container, Kubernetes(k8s) 등과 VM, instance 등이 다양한 개념이 클라우드와 함께 일반인들에게 혼란을 주기도 하는 것 같다. 그런데, 결국 내 밥벌이용이 아니면 같은 철자로 된 단어를 들어도 뜻을 모르는게 당연한거 아닌가 싶다. CD가 누구에게는 Continuous Delivery지만, 누구에게는 Compact Disc, 혹은 Certificate of Deposit 혹은 ….알쥬? 그런 셈이다. 각설하고, 일단 에디터로 돌아가서 Dockerfile을 작성해 보자. 아, 그전에 양해를 구할 것이 있다. 실제 워킹 폴더 명이 회사에서 진행 예정인 중요 프로젝트 명으로 되어 있어서 실제 이름을 적다가 식껍해서 우선 상황으로 설명하는 것으로 변경하였다. 즉, 실제 워킹하지 않는 폴더 명을 Dockerfile, Docker-compose.yml 등에 적었다는 의미이다. 사실 참조의 의미로만 보시는 편이 좋을 듯 하다. 총 3개의 파일이 추가된다: dockerfile 12345678FROM python:3.7-rc-alpine WORKDIR /usr/local/blahblah/ADD requirements.txtRUN pip install --trusted-hosts pypi.python.org -r requirements.txtCMD [&quot;uvicorn&quot;, &quot;main:app&quot;, &quot;--reload&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8000&quot;] FROM은 python 3.7-rc-alpine 버전부터 사용할 수 있다는 것인데, FastAPI는 3.6+ 지원한다. 참고하셈. WORKDIR은 위에 설명해 놨듯, 임의의 폴더를 그냥 쓴거다. 다음에 또 블로그 예제 올리게 되면 그땐 아예 환경 분리해서 따로 만들어서 드리리다 ㅠㅠ 죄송함돠!!! ADD에는 requirements.txt를 로드해서 필요한 라이브러리 등을 적어 놓는다. requirements.txt 123fastapiuvicornwebsockets 처음 빌드할때 websockets을 깜빡해서 왜 에러가 뜨지 왜 안되지 하면서 고민을 잠깐 했었다. 이래서 머리가 나쁘면 손발이 고생한다는 것 같다. docker-compose.yml 123456789version: &quot;3.7&quot;services: api: container_name: &quot;docker-fastapi-websocket&quot; build: . ports: - 8080:8000 volumes: - ./app:/usr/local.//////// 급결론(너무 자세한 설명을 생략했지만서도) 공부하시면 쉽게 접근할 수 있는 내용이라고 믿으면서, docker-compose를 실행하기로 한다. 1docker-compose up --build 실행하고 나면 다음과 같이 브라우저에서 동작하는 것을 볼 수 있을 것이다: 스타트업이나 새로운 프러덕트, 서비스를 개발하고자 하는 곳에서는 파이썬을 많이 사용하고 있다. 실제로도 Machine Learning(ML) 프로젝트나 개발을 하는 곳에서는 자바, C# 등 보다는 파이썬과 관련된 다양한 라이브러리(개인적으로는 PyTorch + PyDantic을 공부 중이다), 프레임워크를 적용하는 경우가 많이 늘은 것 같다. 파이썬의 장점은 쉬운 것이고 단점은 어렵다는 것이다. 파이썬은 루틴, 객체지향, 함수형 언어의 특징이 골고루 들어 있고 다양한 프레임워크와 라이브러리가 산재한다는 것 때문에 쉽게 시작하지만 점점 어려워지는 언어인 것 같다. 파이썬을 사용하면 가장 많이 만나게 되는 프레임워크는 django, flask가 있고 전직장 등에서 django의 장단점을 많이 경험해 보았기에 FastAPI라는 프레임워크가 주는 기대감이 매우 크다. 일단 빠르고, API 기반이란 점이 아주 마음에 든다. 특히 향후 전사 시스템 아키텍쳐를 API, GraphQL, WebSocket or gRPC 등으로 범벅하려는 사악한 계획을 세우고 있기 때문에 더더욱 그렇다. 다음에 기회가 되면 golang으로 테스트하는 내용에 대해서도 정리해 보고 싶기도 하고 FastAPI의 기반이 된 Starlette, Pydantic, 혹은 ASGi에 대한 것도 정리해 보고 싶지만…원체 게을러서 언제 할지는 모르겠다. 일단 줄입니다~ 끄읕!","categories":[{"name":"FastAPI","slug":"FastAPI","permalink":"https://vntgcorp.github.io/categories/FastAPI/"}],"tags":[]},{"title":"ClickUp 유저 매뉴얼","slug":"clickup","date":"2021-06-15T07:05:46.000Z","updated":"2021-11-29T03:57:02.415Z","comments":true,"path":"clickup/","link":"","permalink":"https://vntgcorp.github.io/clickup/","excerpt":"","text":"테디@기술지원실 1. ClickUp 소개 - ClickUp을 이용하여 할 수 있는 것들 2. ClickUp 화면 구성 - 전체 화면의 구성과 역할 3. SPACES - 작업공간의 생성과 설정 관리 4. Lists - List의 생성 및 조회 관리 5. Contents - 리스트의 컨텐츠를 보기 위한 컬럼 생성 관리와 조회 방식 ClickUp 소개 1.1 ClickUp이란? 채팅, 할 일 관리를 위한 리스트 생성, 프로젝트 관리, 칸반 보드, 칸트 차트, 공유 캘린더 등 협업 기능들을 모두 통합하고 , 그 기능들과 외부 기능들 간의 데이터 연동까지 제공하는 문서 기반의 All in One 협업 Tool 페이지 내에서 정리한 프로젝트, 일정, 업무 내용을 칸반 보드, 캘린더, Gantt 차트로 별도 페이지에서 목적에 맞게 한 눈에 파악할 수 있도록 시각적으로 인지시켜 줌. 1.2 ClickUp으로 할 수 있는 것 협업이 필요한 모든 활동들을 Task로 등록하여 다양한 형태의 View로 관리 1.3 목적에 맞는 뷰 서비스 기본적으로 List, Board, Box, Calendar, Gantt, Timeline, Table, 최근 Mind Map까지 총 8가지 타입의 뷰 서비스를 제공한다. 워크플로우를 시각적으로 파악하는 데 용이하다. 업무 관리 리스트를 작성하면 서로의 현황을 쉽게 파악할 수 있어 협업의 효율을 높인다. Task를 다양한 조회와 알람 기능으로 업무 누락 발생을 최소화하고 불필요한 커뮤니케이션을 줄일 수 있다. 출처)문서 기반 올인원 협업툴, ‘클릭업(Click up)’ 1.4 실시간 대화와 비동기 커뮤니케이션 지원❖ 업무 중 방해를 최소화하는 Deep Work를 위한 비동기 커뮤니케이션을 지향❖컨텍스트 스위칭을 최소화하고 실시간 커뮤니케이션과 비동기 커뮤니케이션을 동시에 할 수 있도록 내부 채팅 서비스를 제공❖ 비동기 커뮤니케이션을 위해서는 작업 중인 내용 우측에서 코멘트 기능을 이용하고 실시간 커뮤니케이션이 필요할 때는 클릭업 내부에서 채팅서비스를 이용할 수 있다. 출처)문서 기반 올인원 협업툴, ‘클릭업(Click up)’ 1.5 1000개 이상의 서비스 연동 타임트래킹 앱인 토글(Toggle), 구글 드라이브, 드롭박스, 깃허브, 구글 캘린더, 아웃룩, 슬랙 등 총 1,000개 이상의 외부 서비스가 연동 Coda처럼 클릭업 내에서 작업한 내역을 실시간으로 외부 서비스와 연동하는 기능은 없지만, 클릭 한 번이면 외부 서비스에 데이터를 보낼 수 있다. 출처)문서 기반 올인원 협업툴, ‘클릭업(Click up)’ 1.6 장단점 정리장점 다양한 기능 하나의 리스트로 여러개의 View로 볼 수 있음 리스트 안에서 문서작성 마크다운 기능도 훌륭하고, 자유롭게 컬럼들을 추가, 삭제 가능 Task를 등록하면 다른 Task와 연결 또는 다른 리스트에 등록할 수 있어 모든 기능을 나열할 수 없을 정도 입니다. 외부 다양한 도구들과 연동 Github, Slack, Zapier, 구글드라이브/캘린더 등 1000여개 도구들과 연동이 됩니다. 강한 경쟁력 가격정책이 경쟁사에 비해 월 $5로 착합니다. 빠른 업데이트로 점점 완성도가 높아지고 있습니다. 단점 아쉬운 안전성과 속도 기능이 많아서 그런지 가끔 랙이 걸립니다. 편집기, Mindmap 등 자잘한 버그가 많습니다. 다양한 기능으로 인한 복잡한 사용법 유연함을 추구하여 만들어진 기능들이 조금 어렵습니다. 기능이 많은 것이 복잡해보여 오히려 단점으로 보이는 것이 있습니다. 따로 노는 느낌의 기능들 Doc 기능이 Task와 연결이 아쉽고 Goal 기능이 Task와 별도로 있고, 하나의 리스트에서 각 View 기능마다 설정을 잡아주어야 합니다. 부족한 피드백 시스템 버그를 제보해도 쉽게 개선이 되지 않습니다. 정기적인 업데이트 계획에 따라 개선되어 보입니다. 출처)ClickUp 후기. 범용성은 가장 뛰어난 생산성 도구 ClickUp 화면 구성 2.1 화면 영역 구성 2.2 SIDEBAR Zone - Search ① Search ⓐ 등록되어지는 모든 컨텐츠(Task, 문서, 채팅, 파일, 사용자) 들을 통합하여 검색할 수 있습니다.ⓑ Location을 선택하여 검색되어지는 컨텐츠를 제한할 수 있습니다. 2.3 SIDEBAR Zone - Home ② Home ⓐ 나의 Task들을 상태에 따라 볼 수 있습니다. Comment도 모아서 볼수 있습니다.ⓑ 등록된 Task 중 당일 스케줄을 보여줄 수도 있고, 상단의 Show agenda를 클릭하면 스케줄이 아닌 설정된 아젠다 목록을 볼 수 있습니다. 2.4 SIDEBAR Zone - Notifications ③ Notifications ⓐ 나의 Task들이 설정된 알람에 따라 내용을 확인할 수 있습니다. 또한, Comment된 내용까지 확인할 수 있습니다. 2.5 SIDEBAR Zone - Spaces ④ SPACES ⓐ SPACES를 누르면 아래에 space들이 나열됩니다. 마찬가지로 하나의 space를 클릭하면 하위 폴더나 List 목록이 보여집니다.ⓑ 특정 space, 폴더, 또는 list를 클릭하면 그 안의 하위에 등록된 모든 Task들을 상태에 따라 보여줍니다. 2.6 SIDEBAR Zone - Dashboards ⑤ DASHBOARDS ⓐ DASHBOARDS를 클릭하면 하위 My dashboards와 Shared dashboards로 분리되어 목록이 보여집니다.ⓑ목록에 있는 dashboard를 하나 클릭하면 등록된 Task들의 활동 요약들을 Gragh로 보여집니다. 하나의 그래프 도표는 Widget이라 불리고 상단 Add 버튼을 클릭하면 추가 가능합니다. 2.7 SIDEBAR Zone - Docs ⑥ DOCS ⓐ DOCS를 클릭하면 하위 All, Assigned to me, Shared, Private로 분리되어 목록이 보여집니다.ⓑ 하단의 항목 하나를 클릭하면 작성중이거나 작성이 완료된 것 중 내가 볼수 있는 권한이 있는 문서들이 보여집니다. 2.8 LOCATION &amp; VIEW Setting Zone - View Type❖ List 같은 내용을 Type에 따라 다르게 볼 수 있습니다. List는 상태에 따라 구분되어 목록으로 볼 수 있습니다. ❖ Kanban Board 각 상태의 경계를 구분지어 Task를 두기 때문에 진행 현황을 쉽게 파악할 수 있습니다. ❖ Box 각 담당자들에 배정된 Task와 진행상태를 요약하여 보여줍니다. ❖ Calender Task들의 일정들을 달력으로 보여줍니다. ❖ Gantt Gantt 차트로도 볼 수 있습니다. 이 Gantt 차트 내에서 마우스로 드래그&amp;드롭으로 일정을 변경할 수 있습니다. ❖ Timeline 날짜 기준으로 Task들을 보여줍니다. ❖ Workload 담당자 별 진행 Task를 보여줍니다. (Business Upgrade 필요) ❖ Mindmap Task들의 상하 관계를 보여줍니다. SPACES 3.1 NEW SPACE 생성 3.2 SPACE Setting - Basic SPACE 설정을 언제든지 변경할 수 있습니다. Space 설정 변경을 위한 시작 Space명 우측 [ … ] 선택 Space setting 선택 Edit Space 내 변경 대상 선택 space name Owner Avatar Shared with Task statuses ClickApps Default setting for Views 3.3 SPACE Setting - Options SPACE Option 설정을 언제든지 변경할 수 있습니다. Space 설정 변경을 위한 시작 Space명 우측 [ … ] 선택 Space 대표 색상 변경 Space 대표 Icon 변경 Space 기타 옵션 Duplicate : 현 Space 복사 추가 생성 Template Center : Space 아래 폴더나 List의 빠른 생성을 위해 기 등록되어 있는 template 선택 Custom Fields : 하위 리스트의 속성들을 생성 Leave Space : 선택한 Space에서 나가기 위한 설정 Hide Space : Space 목록에서 보이지 않도록 설정 Delete : 선택한 Space를 완전 삭제함 Automations : Task들의 특정 조건에 자동적으로 알림과 같은 Action을 해주는 것을 설정 Lists 4.1 New List 생성 SPACE Option 설정을 언제든지 변경할 수 있습니다. Space 내 List 추가를 위한 시작 Space명 또는 폴더 우측 [ + ] 선택 New List : Space 아래 생성 New Folder : Space 아래 폴더 생성 Template Center Browse Templates : List의 빠른 생성을 위해 이전에 등록되어 있는 List Template 선택 Save as Template : 용도에 맞게 커스터마이징 된Space, List를 Template Pool에 추가 Update existing Template : 템플릿 Pool에 있는 List Template를 현재의 리스트로 Update하여 수정 New Sprint Folder Sprint에 특화된 List를 생성 Sprint list는 반복 횟수와 기간이 자동 설정되어 생성됨 4.2 List Setting - Sprint List setting Lists Option 설정을 언제든지 변경할 수 있습니다. Space 내 List 설정을 위한 시작 ⓐ 폴더명 또는 List 우측 [ … ] 선택 Sprint setting : List Type이 Sprint 인 경우의 설정 변경 ⓑ Weeks : Sprint 기간 설정 ⓒ Sprint start Day : 출발 요일을 선택 ⓓ Sprint estimation : 스프린트에서 구현 가능한 총 Point ⓔ Sprint name : [Index]는 반복 차수와 [START_DATE-END_DATA] 는 해당 Sprint의 기간을 자동 표시 4.3 List Setting - List status setting Sprint, Folder, List의 Task Status 값을 변경할 수 있습니다. Space 내 List 설정을 위한 시작 Space or 폴더명 or List명 우측 [ … ] 선택 Edit Folder statuses : Task의 진행 상태를 설정하는 기능으로 칸반 구분으로 사용될 수 있습니다. ⓐ Templates : List 성격에 맞게 미리 정의된 상태 Set으로 선택하면 바로 사용 가능 ⓑ Active Status : Task 활동에 대한 시작, 준비, 진행 등의 상태를 정의 ⓒ Done Status : 작업을 완료한 상태의 표시를 정의함 (ex&gt; Review, Test 등) ⓓ Close Status : 최종 끝났을 씨 보여지는 표시를 정의하는 것으로 무조건 존재하는 상태임 ⓔ New template : Status 상태 Set을 Template로 추가합니다. Contents 5.1 List Contents - List View 구성 Contents 영역의 기본 View는 List 입니다. List의 상태 별로 구분되어 Task 목록을 보여줍니다. Contents 목록을 보고싶은 형태로 설정을 해줍니다. 컬럼을 설정한 항목들로 목록에 보여줍니다. 5.2 List Contents - Task 조회 대상 설정 Filter List 내 속성들 값을 선택하여 해당되는 Task만 조회 할 수있습니다. Group by 기본은 Status이지만 Assignee, Priority 및 Custom Field까지 그룹핑 할 수 있습니다. Show Task Locations : Task에 위에 속해있는 List 경로를 보여줄 것을 정합니다. Show closed tasks : 종료된 Tasks를 보여줄 것을 정합니다. Show closed subtasks : 종료된 subtasks를 보여줄 것을 정합니다. Show tasks in multiple Lists : 다른 리스트에도 속한 Task를 보여줄지를 정합니다. Show empty statuses : 상태가 지정되어 있지 않아도 보여줄지를 정합니다. Auto-wrap : subtask들을 자동으로 expand 시키지 않고 접을지를 정합니다. 5.3 List Contents - Task 등록 및 조회 속성 설정 1. Title 영역에도 많은 기능들을 제공합니다. : subtask를 열거나 숨깁니다. : List의 상태 별로 구분되어 Task 목록을 보여줍니다. : subTask 개수를 보여주고, 클릭하면 열거나 숨깁니다. : subTask를 새로 등록할 수 있습니다. : 글 내용이 있음을 알려줍니다. : 글 제목에 Tag를 등록 할 수 있습니다. : 제목을 목록 상태에서 변경할 수 있도록 해줍니다. 2. Column을 클릭하면 설정을 변경할 수 있습니다. Sort: Status 별로 선택된 컬럼을 정렬을 합니다. Sort entire column: 전체 Status를 대상으로 선택된 컬럼을 정렬을 합니다. Move to start: Drag&amp;Drop 가능하지만, 컬럼을 한번에 맨 앞으로 보냅니다. Move to end: 컬럼을 한번에 맨 뒤로 보냅니다. Insert Left: 좌측에 신규 컬럼을 추가할 수 있습니다. Insert Right: 우측에 신규 컬럼을 추가할 수 있습니다. Calculate: 컬럼의 속성이 숫자인 경우 합 계산을 해줍니다. Hide Column: 전체 Status를 대상으로 선택된 컬럼을 정렬을 합니다. Edit Field: 컬럼명을 수정합니다. Duplicate: 같은 형태의 컬럼을 복사하여 추가 생성할 수 있습니다. Delete: 선택된 컬럼을 삭제합니다. 5.4 List Column 추가 ① (+)클릭하여 컬럼을 추가합니다. ② 팝업된 메뉴의 좌측은 List의 기본 컬럼과 이미 생성된 컬럼 목록을 보여줍니다.체크표시는 목록에서 보여주는 컬럼입니다.체크표시가 없는 컬럼은 존재는 하나 목록에서 보여지지 않는 컬럼입니다. ③ 팝업된 메뉴의 우측은 생성할 컬럼의 Type을 보여줍니다.a. Custom field library : 이전에 등록되어 있는 컬럼을 조회할 수 있고 선택만으로 컬럼을 추가 할수 있습니다.b. Custom field library에 없는 경우 컬럼 속성에 맞는 Type을 선택하여 추가 합니다.","categories":[{"name":"ClickUp","slug":"ClickUp","permalink":"https://vntgcorp.github.io/categories/ClickUp/"}],"tags":[]},{"title":"포스트 추가 방법","slug":"newpostmanual","date":"2021-06-09T01:05:46.000Z","updated":"2021-11-29T03:57:02.416Z","comments":true,"path":"newpostmanual/","link":"","permalink":"https://vntgcorp.github.io/newpostmanual/","excerpt":"","text":"배종강@기술지원실안녕하세요. 이번 포스트는 블로그 포스트 추가 방법을 알려드리려고 합니다. 저희는 Git을 통해 소스 코드를 저장하고 편집하는 방법을 사용할것입니다. Git으로 소스 코드를 관리하고 개발 및 편집을 하려면 먼저 저장소를 초기화하거나 원격 저장소를 클론(Clone)해와야 합니다. 클론(Clone)을 사용하면 원격 저장소의 코드를 내 컴퓨터에 받아올 수 있고 로컬 저장소(.git)도 자동으로 생기게 됩니다. 이번글에서는 git clone 명령어의 사용법에 대해서 자세히 소개하고 포스트 추가방법까지 상세히 설명드리겠습니다. 진행하기전 몇가지 설치 및 다운로드를 해야합니다. 아래와 같습니다: 1. Node.js 다운로드 2. Git 다운로드 3. Text Editor (e.g. Visual Studio Code, Atom) 다운로드 이제 포스트 추가 방법을 알려드리겠습니다. 크게 어려운것은 없으니 천천히 따라와주세요. 1. 빈 폴더를 생성해주세요. 이 폴더에 소소코드를 저장할것입니다. 폴더 경로를 잘 기억해주세요. 2. 명령 프롬프트에서 생성한 폴더 directory로 이동후 클론(Clone)명령어를 활용해 Github의 vntg-blog저장소를 받아주세요. –recurse-modules은 submodule인 vntgblogtheme폴더까지 한번에 긁어서 clone해주는 명령어입니다.12cd BlogPostgit clone --recurse-submodules https://github.com/VntgCorp/vntg-blog.git 3. Visual Studo Code나 다른 text-editor로 폴더를 오픈 후 source_posts으로 가서 우클릭후 NewFile을 생성합니다. 파일이름을 적고 .md로 맞춰줍니다. (md는 Markdown 파일이라는것을 알립니다.)4. 포스트 내용 삽입전 포스트 제목, 썸네일, 작성일지, 썸네일 그리고 카테고리(필요시)를 적어줍니다. 5. 포스트 내용 삽입을 해줍니다. 마크다운으로 쉽게 콘텐츠 삽입이 가능합니다. 마크다운 다큐멘테이션 을 통해 이미지는 물론 BlockQuote나 폰트 사이즈도 쉽게 바꿀수 있습니다.6. BlogPost -&gt; vntg-blog 디렉토리에서 npm install을 깔아줍니다.1npm install 7. hexo generate 명령어를 통해 정적 리소스를 생성하고 hexo server 명령어를 통해 로컬 서버에서 테스트 기동을 할 수 있습니다. hexo generate 명령어를 실행하면 public 폴더가 생성되면서 정적 리소스가 생성됩니다. 로컬 서버에서 테스트전에 먼저 hexo generate을 해주어야 합니다.12hexo generatehexo server 8. 깃허브 페이지에 배포하기.배포하기전 _config.yml파일에 들어가서 배포할 저장소가 맞는지 확인합니다. 저희 공식 기술블로그 repo는 아래와 같습니다. 1234deploy: type: git repo: https://github.com/VntgCorp/VntgCorp.github.io.git branch: master 마지막으로 최종배포전 정적 리소스를 다시 한번 생성합니다. 이 파일들이 실제로 깃허브 페이지에 배포될 파일들입니다. 1hexo generate 아래 명령어를 사용하면 생성한 정적 리소스가 배포됩니다. VntgCorp.github.io 주소로 들어가면 정상적으로 배포된 것을 확인할 수 있습니다. 1hexo deploy 정적 리소스를 삭제하는 명령어는 아래와 같습니다. 간혹 정상적으로 배포가 되었는데도 불구하고 페이지가 업데이트 되지 않는 현상이 생기면 리소스를 삭제하고 다시 생성하시면 됩니다. 그래도 안되면 캐시 비우기와 강력 새로고침을 합니다. 1hexo clean 9. 깃허브 저장소에 소스 저장하기위의 배포 과정은 보여지는 페이지를 저장한 것이고, 이제 실제 소스와 이미지들을 깃허브 저장소에 저장해야합니다. 그래야 다른분이 포스트 추가가 가능할테니까요. 아래 코드를 입력하시면 소스들이 처음에 클론 받았던 저장소로 푸쉬된 것을 확인할수 있습니다. 12345git initgit remote add origin https://github.com/VntgCorp/vntg-blog.gitgit add .git commit -m &#x27;커밋 메시지&#x27; git push origin master 간혹 깃의 히스토리가 충돌되어 푸쉬가 안되는 경우가 있는데 이런 상황에는 아래 명령어를 사용하여 해결해 주시면 됩니다. 1git pull origin master --allow-unrelated-histories 직접 테마를 수정하지 않는 이상 테마 저장소는 건드리지 않습니다. 주의사항:저희 기술블로그는 Hexo를 기반으로 만든 페이지입니다. 저희 Hexo 블로그는 3개의 저장소(repo)를 사용합니다. 한개는 배포 페이지를 저장하는 저장소로 사용되며 다른 한개는 실제 소스를 저장하는 저장소로 사용됩니다. 마지막으로 테마 페이지는 저희 기술블로그 테마를 클론할때만 사용합니다. 클론/푸쉬/커밋등을 하는 중에 다른 저장소를 건드리면 복구가 힘들어지고 골치가 아플 수 있습니다. (저처럼.. ㅠㅠ)항상 주의깊게 어떤 저장소를 건드리는지 확인하시면 좋겠습니다.","categories":[{"name":"Static Site Generator","slug":"Static-Site-Generator","permalink":"https://vntgcorp.github.io/categories/Static-Site-Generator/"}],"tags":[]},{"title":"Hexo & Gatsby 차이점(주관적)","slug":"hexoVSgatsby","date":"2021-06-03T04:05:46.000Z","updated":"2021-11-29T03:57:02.416Z","comments":true,"path":"hexoVSgatsby/","link":"","permalink":"https://vntgcorp.github.io/hexoVSgatsby/","excerpt":"","text":"배종강@기술지원실1. Setup Process Hexo Gatsby 난이도 낮음 비교적 있음 로딩속도 빠름 느림 2. Developer Experience Hexo Gatsby 블로그 구축 Tag Plugin을 통해 쉽게 간단한 컨텐츠 구축 비교적 더 raw함.Javascript에 대한 이해도 더 필요 커스터마이징 약간의 커스터마이징 가능 React와 Query Language를 통해 더 다이나믹한 커스터마이징 가능 플러그인 250개 이상 500개 이상 3. Community &amp; Documentation Hexo Gatsby 커뮤니티 80%가 중국커뮤니티 Hexo보다 더 넓은 커뮤니티 보유. 대부분 영어권 다큐멘테이션 한국어버전 보유 대부분 영어 4. Github Deploy Hexo Gatsby 배포속도 빠름 비교적 느림 과정 One-Command Deployment을 통해 바로 배포가능 소스코드에서 코드삽입후 배포 총평:HEXO: 마크업블로그에 최적화. 쉽고 빠른 블로그에 적합.GATSBY: 커스터마이징에 최적화. Beginner들한테 다소 어렵고 복잡할수 있음.","categories":[{"name":"Static Site Generator","slug":"Static-Site-Generator","permalink":"https://vntgcorp.github.io/categories/Static-Site-Generator/"}],"tags":[]},{"title":"Hexo 설치 및 사용 매뉴얼","slug":"hexo","date":"2021-06-03T02:05:46.000Z","updated":"2021-11-29T03:57:02.415Z","comments":true,"path":"hexo/","link":"","permalink":"https://vntgcorp.github.io/hexo/","excerpt":"","text":"배종강@기술지원실1.초기설정 Node.js 다운로드 명령프롬프트(Command Line/CLI) 에서 npm install -g hexo-cli 입력1npm install -g hexo-cli Hexo 파일 폴더 생성후 페이지 생성 확인1234hexo init [폴더명]cd [폴더명]npm installhexo server 2. 테마설정(Hueman 테마이용) 해당폴더로 이동후 Hueman 테마를 themes/hueman 폴더로 클론.12cd [폴더명]git clone https://github.com/ppoffice/hexo-theme-hueman.git themes/hueman config.yml 에서 ##Theme 부분을 landscape에서 hueman으로 수정.12## Themes: https://hexo.io/themes/theme: hueman themes/hueman 폴더에 있는 _config.yml.example를 _config.yml로 바꿈. 3.커스터마이징 Gatsby 블로그와 유사하게 _config.yml에서 제목 및 저자 수정 가능12345678 # Sitetitle: VNTG 기술블로그 subtitle: Technical Blogdescription: &#x27;&#x27;keywords:author: VNTGlanguage: entimezone: &#x27;&#x27; 4. Github 저장소에 소스 저장(Push) 새로운 저장소(repo)를 만들고 소스들을 푸쉬해준다.12345git initgit remote add origin 저장소git add .git commit -m “커밋 메세지”git push origin master 5. Github 페이지에 배포 _config.yml으로 이동후 deploy를 아래 코드와 같이 변경 1234deploy: type: git repo: https://github.com/xxxx/xxxx.github.io.git branch: master 배포를 가능케하는 플러그인 (hexo-deployer-git)을 설치 1npm install --save hexo-deployer-git 정적 리소스를 생성후 배포 12hexo generatehexo deploy Reference","categories":[{"name":"Static Site Generator","slug":"Static-Site-Generator","permalink":"https://vntgcorp.github.io/categories/Static-Site-Generator/"},{"name":"Hexo","slug":"Static-Site-Generator/Hexo","permalink":"https://vntgcorp.github.io/categories/Static-Site-Generator/Hexo/"}],"tags":[]},{"title":"Gatsby 설치 및 사용 매뉴얼","slug":"manual","date":"2021-06-03T01:05:46.000Z","updated":"2021-11-29T03:57:02.416Z","comments":true,"path":"manual/","link":"","permalink":"https://vntgcorp.github.io/manual/","excerpt":"","text":"배종강@기술지원실Gatsby1.초기설정 Node.js 다운로드 명령프롬프트(Command Line/CLI) 에서 npm install -g gatsby-cli 입력1npm install -g gatsby-cli 템플릿 클론 해당 홈페이지에 많은 템플릿들을 확인후 선택. 템플릿 고르고 github repository 링크를 clone CLI에서 클론한 링크 입력후 페이지 생성 확인 123gatsby new my-project https://xxxxxxxcd my-projectgatsby develop 2.커스터마이징 gatsby-config.js으로 접속후 제목/저자/테마배경컬러등을 바꿀수 있음 123456789options: &#123; name: `VNTG`, short_name: `VNTG`, start_url: `/`, background_color: `#f7f0eb`, theme_color: `#a2466c`, display: `standalone`, icon: &quot;static&quot; + settings.meta.iconimage,&#125;, 3.포스트 추가하기 포스트는 우리가 gatsby-config.js 파일에서 별다른 설정을 하지 않았기 때문에 디폴트로 content/posts 디렉터리 내에 만들면 자동으로 블로그 포스트 목록에 표시됨. 템플릿은 똑같이 입력하고 제목이나 slug 추가후 내용삽입하면 완성12345678---template: blog-posttitle: 예시slug: /newblogpagedate: 2021-06-03 10:46description: 예시포스트--- 4. Github 배포 GitHub Pages 는 GitHub에서 무료로 호스팅하는 공개 웹페이지.GitHub의 저장소에서 개인이나 조직 또는 프로젝트 페이지를 호스팅하도록 설계됨. GitHub Pages 의 종류는 크게 두가지 개인 사이트 GitHub Pages 프로젝트 사이트 GitHub Pages 개인 사이트는 (username).github.io라는 저장소를 만들게되면 자동으로 개인 사이트를 위한 GitHub Pages를 만들 수 있음. 이 사이트의 저장소 브랜치는 master에서 만들어져야함. 프로젝트 사이트 GitHub에 등록한 프로젝트별로 사이트를 자동으로 만들 수 있음. 프로젝트 사이트는 (username).github.io/[프로젝트 저장소 이름]으로 만들어짐. 사이트의 저장소를 위한 브랜치가 gh-pages라는 브랜치이여야함. 배포방법 github에서 ‘owner이름/owner이름github.io’ repository 생성e.g) https://github.com/배종강/배종강.github.io.git CLI에서 gh-pages 설치 1npm install gh-pages --save-dev package.json에서 gh-pages 코드 추가 12345 &#123; &quot;scripts&quot;: &#123; &quot;deploy&quot;: &quot;gatsby build --prefix-paths &amp;&amp; gh-pages -d public&quot; &#125;&#125; CLI에서 코드를 Github 해당 repo로 push후 배포1234567git initgit remote add origin (github repo 주소)git add .git commit -m &quot;커밋내용&quot;git push -u origin gh-pagesnpm run deploy Reference","categories":[{"name":"Static Site Generator","slug":"Static-Site-Generator","permalink":"https://vntgcorp.github.io/categories/Static-Site-Generator/"},{"name":"Gatsby","slug":"Static-Site-Generator/Gatsby","permalink":"https://vntgcorp.github.io/categories/Static-Site-Generator/Gatsby/"}],"tags":[]},{"title":"기존 버전관리 및 CICD 현황","slug":"CICD","date":"2021-05-31T07:05:46.000Z","updated":"2021-11-29T04:21:01.316Z","comments":true,"path":"CICD/","link":"","permalink":"https://vntgcorp.github.io/CICD/","excerpt":"","text":"TJ@기술지원실소개: 이 문서는 github, gitlab의 CICD 기능을 테스팅하고 검토하면서 정리한 자료입니다. &nbsp; 선택 : 설치형 vs SaaS (Software as a Service) Gitlab은 모든 타입에서 설치형 지원 Github는 Enterprise 에서만 설치형 지원 IDC나 사내에서 CICD 를 쓸 일이 있으면 필요한 곳에 runner 를 설치하면 가능 &nbsp; Gitlab vs Github 가격비교 table { font-family: arial, sans-serif; border-collapse: collapse; width: 100%; } td, th { border: 2px solid #000000; text-align: left; padding: 8px;} Gitlab Github https://about.gitlab.com/pricing/ https://github.com/pricing Free Free Premium : 1인당 월$19*12= 년 $228 Team : 1인당 월 $4*12 = 년 $48 Ultimate : 1인당 월$99*12=년 $1188 Enterprise : 1인당 월 $21*12 = 년 $252 &nbsp; Gitlab vs Github 기능 비교타입별 기능 비교:Free 버전에서는 git 사용을 위한 기본적인 기능만 지원을 하며 기술지원 없음PR(MR) 요청을 했을 때 리뷰어를 두고 승인하는 기능은 Free 버전에는 없으며 Gitlab Premium이나 Github Team을 써야 함Gitlab Ultimate, Github Enterprise 는 주로 보안과 관련한 기능 추가 일반적인 기능은 비슷하게 지원이 됨 버전관리 프로젝트관리(wiki, issues) 패키지기능(npm, RubyGems, Apache Maven, Gradle, Docker, and NuGet, docker) CICD API 연동 Gitlab은 CICD 기능 외에 다양한 부가 기능을 지원 Monitor application 퍼포먼스 메트릭으로 신뢰성 및 안정성 측정 경고 및 사고 관리 애플리케이션의 오류 추적 애플리케이션 상태 및 성능 추적 로그 집계 및 저장 코드로 인프라 관리 (terraform remote backend) k8s 통합기능 테스팅과 관련한 각종 툴이 통합 : 단위테스트 보고서, 접근성 테스트, 브라우저 성능 테스트, 코드 품질, 부하 성능 테스트, 메트릭 레포트 Unit 테스트 리포트 Gitlab에서는 여러 가지 부가기능을 많이 지원하지만 Github 에서는 다른 대체수단을 이용하여 처리할 수 있을 것으로 생각됨 terraform remote backend 는 AWS 에서 S3+dynamoDB 와 연동하여 처리 가능 k8s는 Flux 나 Argo CD 등을 이용 CICD는 Github 에서 Github actions 를 지원하면서 큰 차이가 없음 Github marketplace 에서 필요한 것을 찾거나 직접 만들어서 사용 하면 됨 Gitlab, Github CICD 예제 Gitlab CI/CD 예제 Github 액션 가이드 Gitlab CICD의 장점은 environments 를 이용하여 각 환경(staging, prod)에 대한 배포를 손쉽게 하고 기록을 확인가능한 점 Github 은 environments 기능을 public repo나 enterprise plan 일 경우에만 지원하고 github team 의 private repo에는 지원 안함 Github 에서 environments 기능은 리뷰어의 승인 이 있어야 배포 가능/트리거 발생후 일정 시간이 지나야 배포 가능/특정 브랜치만 배포 가능 &nbsp; Github 구성 IDC에서 CICD를 하는 경우에는 IDC에 윈도우용 runner, LInux runner 구성 AWS 를 이용하는 경우에는 소스코드 관리는 Github로 하되 AWS CodePipeline 를 이용할 수도 있음. 소스코드링크 Github Actions Quickstart 에서 간단한 github actions 직접 만들어서 해봄. 액션링크 Guides 에서 주로 사용하는 언어를 골라서 build, test, deploy 하는 예제를 살펴본다. 액션가이드 Github Actions 배우기 Learn GitHub Actions : github actions 를 이용하여 workflow 를 만드는 방법을 상세히 설명. 액션배우기 Github actions 과정에 대한 설명, actions 찾기, 조직과 워크플로우 공유하기, 보안 등을 다루고 있음. Github Actions 만들기 Actions는 jobs을 만들고 workflow를 구성하는 개별 작업으로 자신이 직접 만들거나 github 커뮤니티에서 다른 사람들이 만든 것을 이용할 수 있다. 자세히 Actions는 3가지 타입으로 만들 수 있으며 Docker container, JavaScript, Composite run steps 가 있다. 자세히 Docker container 는 리눅스만 지원한다. JavaScript는 runner 에서 실행이 되며 “JavaScript 작업을 사용하면 작업 코드가 단순화되고 Docker 컨테이너 작업보다 빠르게 실행됩니다.” 라고 문서에 나와있다. Composite run steps 는 쉘스크립트를 이용하여 처리하는 경우로 Docker container, JavaScript 보다는 뒤에 기능이 추가가 되었다. Reference Encrypted secrets : 암호화된 비밀정보. workflow 파일에서 input 이나 environment 변수로 받아서 사용할 수 있음.자세히 Environments 를 이용하면 배포시 몇가지 제한을 걸 수 있다. 리뷰어의 승인이 있어야 배포 가능, 트리거 발생후 일정 시간이 지나야 배포 가능, 특정 브랜치만 배포 가능. 그런데 이 기능은 공개 repo이거나 enterprise plan 일 경우에만 지원을 하고 github team 의 private repo에는 아직 지원 안함. 자세히 Environments 를 이용한 예제 Pull Request 상태 일 때만 Dev에 배포 메인에 커밋하거나 병합 할 때만 스테이징에 배포 스테이징 후 승인을 받아 프로덕션에 배포 Github Actions 제한자세히 Job execution time : 6시간Workflow run time : 72 hours Github self-hosted runnersGithub runner 설치 및 등록Repository 또는 organization 차원에서 runner 를 등록할 수 있음. 자세히 runner 를 추가하는 화면에서 runner를 설치하고 설정하는 부분에 대한 설명이 나옴. 설치시 root로 진행을 하면 sudo 관련한 에러가 나옴. 보안을 위해서 별도 user를 만들어서 runner를 실행함. runner 를 실행할 user를 만들고 docker 를 사용하기 위해서 docker group에 추가를 해준다. 123useradd -m -s /bin/bash --comment &#x27;github runner&#x27; github-runnerusermod -aG docker github-runnersudo su - github-runner 1234deploy: type: git repo: https://github.com/VntgCorp/VntgCorp.github.io.git branch: master Github-runner user로 runner 설치. 설정을 할 때 나오는 url과 token 정보를 이용:1234$ mkdir actions-runner &amp;&amp; cd actions-runner$ curl -o actions-runner-linux-x64-2.278.0.tar.gz -L https://github.com/actions/runner/releases/download/v2.278.0/actions-runner-linux-x64-2.278.0.tar.gz$ tar xzf ./actions-runner-linux-x64-2.278.0.tar.gz.$ /config.sh --url https://github.com/VntgCorp --token ATZHRXXXXXXXX 아래는 root 권한으로 실행을 해야 함. systemd 설정을 하며 실행하는 사용자를 지정하는 것임:123# ./svc.sh install github-runner# ./svc.sh start# ./svc.sh status Self-hosted runners 를 github actions에 추가할 때 자동으로 self-hosted, 운영 체제 및 하드웨어 플랫폼 레이블이 설정 된다. 필요하면 추가 레이블을 설정할 수 있다. 예를 들어 x86_64 ubuntu 20.04 의 경우 self-hosted, linux, X64 로 나온다. 보안 문제로 public repo의 경우에는 기본적으로 organization에 있는 runner group 을 사용할 수 없다.자세히 workflow 에서 runner 사용: 자세히 runs-on: [self-hosted, linux, X64]. workflow에서 runner에 할당한 레이블을 넣으면 됨. (and 조건임) Github actions 기타 참고자료 Docker registry, container registry자세히 123$ docker login https://docker.pkg.github.com -u tjmoon-vntg$ docker build -t docker.pkg.github.com/tjmoon-vntg/action_test/docker_test:1.0 .$ docker push docker.pkg.github.com/tjmoon-vntg/action_test/docker_test:1.0 현재는 package namespace 를 사용하는 Docker registry 를 지원하고 있는데 향후에는 Container Registry 로 대체를 할 것이라고 함. Container Registry 는 사용자 또는 조직에 대해서 설정을 하며 아직은 베타버전임. 자세히 &nbsp; Gitlab 구성 Gitlab runner 설치 및 등록Runner type : shared runners, group runners, specific runners OS에 gitlab-runner 설치.executor 가 여러 가지 있는데 docker 를 추천하고 있음. OS에 docker 설치.gitlab-runner 설치 후 runner 를 등록함. executor docker로 실행시 gitlab.example.com 을 찾지 못함. 12345678910111213REGISTRATION_TOKEN=gCGg8tkK51TATVpjsBHzsudo gitlab-runner register \\ --non-interactive \\ --url http://gitlab.example.com/ \\ --registration-token $REGISTRATION_TOKEN \\ --executor &quot;docker&quot; \\ --docker-image alpine:latest \\ --description &quot;docker-runner&quot; \\ --tag-list &quot;docker,aws&quot; \\ --run-untagged=&quot;true&quot; \\ --locked=&quot;false&quot; \\ --access-level=&quot;not_protected&quot; extra_hosts 옵션을 추가해서 docker 에서 /etc/hosts를 업데이트할 수 있음.extra_hosts = [“other-host:127.0.0.1”] 자세히 123456789101112131415161718192021cat &gt; /tmp/test-config.template.toml &lt;&lt; EOF[[runners]]name = &quot;docker-runner&quot;[runners.docker]extra_hosts = [&quot;gitlab.example.com:192.168.33.12&quot;]EOFREGISTRATION_TOKEN=gCGg8tkK51TATVpjsBHzsudo gitlab-runner register \\ --non-interactive \\ --url http://gitlab.example.com/ \\ --registration-token $REGISTRATION_TOKEN \\ --executor &quot;docker&quot; \\ --docker-image alpine:latest \\ --description &quot;docker-runner&quot; \\ --tag-list &quot;docker,aws&quot; \\ --run-untagged=&quot;true&quot; \\ --locked=&quot;false&quot; \\ --access-level=&quot;not_protected&quot; \\ --template-config /tmp/test-config.template.toml shell로 등록시 아래 에러가 나왔음. gitlab-runner verify 명령으로 처리.‘New runner. Has not connected yet.’ 에러 CICD pipeline stage 는 정의를 하지 않으면 기본으로 5가지가 사용된다. (pre, build, test, deploy, post)자세히 runner 에서 동시작업을 하려고 하면 gitlab-runner에서 concurrent 옵션을 바꾸어준다. 기본은 1로 되어 있다. 이 부분을 바꾸어 주어야 여러개의 job을 동시에 실행할 수 있다. 자세히 pipeline이 성공했을 때만 Merge하는 기능은 유용할 듯. ssh 이용하는 경우 다음 내용 참고. 자세히 environments 이용하여 배포 기록 남기고 과거 버전으로 롤백 할 수 있음. 기타, 다른 툴과 연동terraform 의 remote state 를 저장하는 용도로 사용 가능함. terraform을 여러 명이 함께 사용을 하는 경우 필요한 기능임. 자세히 https://docs.gitlab.com/ee/administration/terraform_state.html Github, Gitlab CICD environments 기능 비교gitlab environments 예제environment 를 활용하는 간단한 예제이다. (.gitlab-ci.yml) 사전에 environment 에 staging, production 을 구성한다. 소스코드가 업데이트되면 ubuntu:20.04 docker 컨테이너에서 실행이 되며 staging 은 ssh를 이용하여 자동 배포가 된다. production 에는 수동으로 배포를 하도록 구성을 했다. (manual) 수동으로 배포를 할 수 있는 사람도 지정할 수 있다. 자세히 이제 environment 로 가면 staging, production 환경별로 배포한 기록을 볼 수 있고 특정 commit 으로 롤백을 할 수도 있다. 1234567891011121314151617181920212223242526272829303132image: ubuntu:20.04before_script: - apt-get update - &#x27;which ssh-agent || apt-get install openssh-client -y&#x27; - mkdir -p ~/.ssh - eval $(ssh-agent -s) - &#x27;[[ -f /.dockerenv ]] &amp;&amp; echo -e &quot;Host *\\n\\tStrictHostKeyChecking no\\n\\n&quot; &gt; ~/.ssh/config&#x27;deploy-staging: stage: deploy script: - echo &quot;staging, This job deploys something from the $CI_COMMIT_BRANCH branch.&quot; - ssh-add &lt;(echo &quot;$STAGING_PRIVATE_KEY&quot;) - ssh -p22 test@192.168.33.14 &quot;mkdir -p test&quot; - ssh -p22 test@192.168.33.14 &quot;date ; touch a.txt&quot; - scp -P22 * test@192.168.33.14:test/ - scp -P22 index.html test@192.168.33.14:/var/www/html/index.html environment: name: staging url: http://staging.example.comdeploy-production: stage: deploy script: - echo &quot;prd , This job deploys something from the $CI_COMMIT_BRANCH branch.&quot; environment: name: production url: http://www.example.com when: manual only: - master &nbsp; Gitlab pipeline &nbsp; Gitlab environments &nbsp; Gitlab staging &nbsp; github environments 예제Github 은 environments 기능을 public repo나 enterprise plan 일 경우에만 지원하고 github team 의 private repo에는 지원 안함Github 에서 environments 기능은 리뷰어의 승인이 있어야 배포 가능/트리거 발생후 일정 시간이 지나야 배포 가능/특정 브랜치만 배포 가능https://docs.github.com/en/actions/reference/environments 아래 github workflow 파일은 다음과 같은 내용을 담고 있다. 사전에 production environments 에 Required reviewers 설정을 함. Pull Request 상태 일 때만 Dev에 배포 메인에 커밋하거나 병합 할 때만 스테이징에 배포 스테이징 후 승인을 받아 프로덕션에 배포. 해당 environments 의 Required reviewers 가 승인을 해 주어야 배포가 됨. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# https://dev.to/n3wt0n/everything-you-need-to-know-about-github-actions-environments-9p7name: CI + CDon: push: branches: [ main ] pull_request: branches: [ main ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch:jobs: Build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Compile run: echo Hello, world! DeployDev: name: Deploy to Dev if: github.event_name == &#x27;pull_request&#x27; needs: [Build] runs-on: ubuntu-latest environment: name: Development url: &#x27;http://dev.example.com&#x27; steps: - name: Deploy run: echo I am deploying! DeployStaging: name: Deploy to Staging if: github.event.ref == &#x27;refs/heads/main&#x27; needs: [Build] runs-on: ubuntu-latest environment: name: Staging url: &#x27;http://staging.example.com/&#x27; steps: - name: Deploy run: echo I am deploying! DeployProd: name: Deploy to Production needs: [DeployStaging] runs-on: ubuntu-latest environment: name: Production url: &#x27;http://www.example.com&#x27; steps: - name: Deploy run: echo I am deploying! &nbsp; Github dev &nbsp; Github staging &nbsp; Github staging-log","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://vntgcorp.github.io/categories/DevOps/"},{"name":"CICD","slug":"DevOps/CICD","permalink":"https://vntgcorp.github.io/categories/DevOps/CICD/"}],"tags":[]},{"title":"Git의 대표적인 3가지 workflow","slug":"Git","date":"2021-05-31T06:05:46.000Z","updated":"2021-11-29T03:57:02.415Z","comments":true,"path":"Git/","link":"","permalink":"https://vntgcorp.github.io/Git/","excerpt":"","text":"찰리브라운@기술지원실 Git의 대표적인 3가지 workflow, Gitflow / Github flow / Gitlab flow 개발언어(특히나 JavaScript관련)만큼 변화가 많은건 아니지만, Version Control System도 긴 역사끝에 현재는 Git의 전성기가 되었습니다. PVCS, SVN도 어느덧 추억의 이름이 되어가고... VCS로 Git을 도입함에 따라(Gitlab/Github의 결정이 남아있지만) 어떤 workflow를 도입할지도 결정이 필요합니다. 대표적인 3가지 workflow의 특징을 간략하게 정리합니다. Gitflow 5개로 가장 많은 branch가 존재하며 그만큼 복잡도가 높음 table { font-family: arial, sans-serif; border-collapse: collapse; width: 100%; } td, th { border: 2px solid #000000; background-color: white; text-align: left; padding: 8px; } tr:nth-child(even) { background-color: white; } Feature Develop Release Hotfix Master 임시 기반 임시 임시 기반 Master 중심이 되는 2개의 branch 중 하나. 최종 릴리즈에 사용되는 branch로 태깅을 통해 버전관리 Develop 중심이 되는 또다른 branch. 차기 릴리즈를 위한 개발의 메인으로 추가 기능 개발시 Develop에서 Feature branch를 생성 개발이 완료된 기능은 Develop branch로 merge Feature Feature branch는 origin에 반영되는게 아닌, 개발자의 repo에 존재 Merge가 완료되면 branch는 삭제 Release기능개발이 완료되어 차기 릴리즈를 위한 준비가 되면 Develop branch에서 생성 이 시점부터 Develop branch에는 차차기 릴리즈를 위한 개발이 가능 Release branch에는 버그픽스를 위한 수정만 커밋되고, 버그픽스가 완료되면 Master branch, Developer branch의 두곳에 merge Master branch에 tagging을 통해 릴리즈버전을 기록 HotfixProduction환경에서 버그가 발생하면 Master branch에서 생성.수정 완료 후 Master branch, Developer branch의 두곳에 merge Master branch에 tagging을 통해 핫픽스 릴리즈버전을 기록 Release branch가 있다면 해당 branch에도 merge 필요 GitFlow는 실제 운용이 복잡 GitFlow를 보고있으니 자연스럽게 merge전투가 떠오릅니다 Github flow Master (작업용branch) 기반 임시 Master branch만 stable 하다면, 작업용 임시 branch에 대해서는 특별한 규칙이 없는 방식Develop branch가 따로 존재하지 않으며, Feature와 Hotfix 등 모든게 Master branch에서 생성되고 작업완료 후 Master branch로 merge됨 merge할때 무조건 pull request가 필요하며, 코드에 대한 다른 개발자의 리뷰를 통해 master 이렇다보니 CI는 선택이 아닌 필수 Master branch로 merge시 즉시 배포까지 이어지게 구성되어야 한다 배포가 자동화 되어있지 않은 상황에서 도입은 현실적으로 어렵다 Gitlab flow table { } td, th { border: 2px solid #dddddd; text-align: left; padding: 8px;} tr:nth-child(even) { background-color: white;} Master Pre-production Production 기반 기반 기반 &nbsp; Gitlab에서는 하나가 아닌 몇가지 flow를 제시하고 있는데, 그중 대표적인 Environment branches with Gitlab flow를 리뷰 Gitlab의 Production branch는 Gitflow의 Master branch 역할과 동일Pre-production branch는 Master → Production으로 바로 반영하지 않고 staging과 production의 중간버퍼 단계로 추가됨 Gitlab의 Master branch는 Pre-production branch, Production branch로 일방적인 deploy 만 진행 Master branch는 계속 변화가 가능 Width 변경을 위한 코드 추가 Theme 파일 수정","categories":[{"name":"Git","slug":"Git","permalink":"https://vntgcorp.github.io/categories/Git/"}],"tags":[]}],"categories":[{"name":"Test","slug":"Test","permalink":"https://vntgcorp.github.io/categories/Test/"},{"name":"Data","slug":"Data","permalink":"https://vntgcorp.github.io/categories/Data/"},{"name":"Metabase","slug":"Data/Metabase","permalink":"https://vntgcorp.github.io/categories/Data/Metabase/"},{"name":"Level2","slug":"Data/Level2","permalink":"https://vntgcorp.github.io/categories/Data/Level2/"},{"name":"Grafana","slug":"Data/Grafana","permalink":"https://vntgcorp.github.io/categories/Data/Grafana/"},{"name":"DevOps","slug":"DevOps","permalink":"https://vntgcorp.github.io/categories/DevOps/"},{"name":"Terraform","slug":"DevOps/Terraform","permalink":"https://vntgcorp.github.io/categories/DevOps/Terraform/"},{"name":"Docker","slug":"Docker","permalink":"https://vntgcorp.github.io/categories/Docker/"},{"name":"FastAPI","slug":"FastAPI","permalink":"https://vntgcorp.github.io/categories/FastAPI/"},{"name":"ClickUp","slug":"ClickUp","permalink":"https://vntgcorp.github.io/categories/ClickUp/"},{"name":"Static Site Generator","slug":"Static-Site-Generator","permalink":"https://vntgcorp.github.io/categories/Static-Site-Generator/"},{"name":"Hexo","slug":"Static-Site-Generator/Hexo","permalink":"https://vntgcorp.github.io/categories/Static-Site-Generator/Hexo/"},{"name":"Gatsby","slug":"Static-Site-Generator/Gatsby","permalink":"https://vntgcorp.github.io/categories/Static-Site-Generator/Gatsby/"},{"name":"CICD","slug":"DevOps/CICD","permalink":"https://vntgcorp.github.io/categories/DevOps/CICD/"},{"name":"Git","slug":"Git","permalink":"https://vntgcorp.github.io/categories/Git/"}],"tags":[]}